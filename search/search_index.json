{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud Native 101 Workshop \u00b6 Welcome to the Cloud Native 101 Workshop! In the workshop you will learn about foundational open source technologies behind the cloud native application development. You will also learn about deploying a simple application on OpenShift based Kubernetes cluster running on IBM Cloud. May 7 th - 8am to 12pm EST Welcome, Introductions & Objectives All 10 mins Lecture: Overview of Docker with demo John Zaccone 30 mins Homework: Hands-On Docker BREAK (8:40 am) 10 mins Lecture: Overview of Kubernetes John Zaccone 30 mins Lab: Kubernetes 101 John Zaccone 60 mins BREAK (10:20 am) 10 mins Lecture: Helm Rojan Jose 30 mins Lab: Helm 101 Rojan Jose 45 mins Wrap up Eric Andersen 15 mins Next steps: \u00b6 Environment availability What would you like to learn next? Think 2021: On demand sessions Hands-on Labs Feedback Technologies \u00b6 Docker Kubernetes Helm Red Hat OpenShift Kubernetes Service Credits \u00b6 John Zaccone Rojan Jose","title":"Workshop Overview"},{"location":"#cloud-native-101-workshop","text":"Welcome to the Cloud Native 101 Workshop! In the workshop you will learn about foundational open source technologies behind the cloud native application development. You will also learn about deploying a simple application on OpenShift based Kubernetes cluster running on IBM Cloud. May 7 th - 8am to 12pm EST Welcome, Introductions & Objectives All 10 mins Lecture: Overview of Docker with demo John Zaccone 30 mins Homework: Hands-On Docker BREAK (8:40 am) 10 mins Lecture: Overview of Kubernetes John Zaccone 30 mins Lab: Kubernetes 101 John Zaccone 60 mins BREAK (10:20 am) 10 mins Lecture: Helm Rojan Jose 30 mins Lab: Helm 101 Rojan Jose 45 mins Wrap up Eric Andersen 15 mins","title":"Cloud Native 101 Workshop"},{"location":"#next-steps","text":"Environment availability What would you like to learn next? Think 2021: On demand sessions Hands-on Labs Feedback","title":"Next steps:"},{"location":"#technologies","text":"Docker Kubernetes Helm Red Hat OpenShift Kubernetes Service","title":"Technologies"},{"location":"#credits","text":"John Zaccone Rojan Jose","title":"Credits"},{"location":"generatedContent/","text":"This content is generated! Do not edit directly! Please run aggregate-labs.sh to repopulate with latest content from agenda.txt!","title":"Index"},{"location":"generatedContent/helm101/","text":"Helm 101 \u00b6 Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly? Objectives \u00b6 These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing Prerequisites \u00b6 Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm. Helm Overview \u00b6 Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details. Helm Abstractions \u00b6 Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"Helm 101"},{"location":"generatedContent/helm101/#helm-101","text":"Helm is often described as the Kubernetes application package manager. So, what does Helm give you over using kubectl directly?","title":"Helm 101"},{"location":"generatedContent/helm101/#objectives","text":"These labs provide an insight on the advantages of using Helm over using Kubernetes directly through kubectl . In several of the labs there are two scenarios. The first scenario gives an example of how to perform the task using kubectl , the second scenario, using helm . When you complete all the labs, you'll: Understand the core concepts of Helm Understand the advantages of deployment using Helm over Kubernetes directly, looking at: Application management Updates Configuration Revision management Repositories and chart sharing","title":"Objectives"},{"location":"generatedContent/helm101/#prerequisites","text":"Have a running Kubernetes cluster. See the IBM Cloud Kubernetes Service or Kubernetes Getting Started Guide for details about creating a cluster. Have Helm installed and initialized with the Kubernetes cluster. See Installing Helm on IBM Cloud Kubernetes Service or the Helm Quickstart Guide for getting started with Helm.","title":"Prerequisites"},{"location":"generatedContent/helm101/#helm-overview","text":"Helm is a tool that streamlines installation and management of Kubernetes applications. It uses a packaging format called \"charts\", which are a collection of files that describe Kubernetes resources. It can run anywhere (laptop, CI/CD, etc.) and is available for various operating systems, like OSX, Linux and Windows. Helm 3 pivoted from the Helm 2 client-server architecture to a client architecture. The client is still called helm and, there is an improved Go library which encapsulates the Helm logic so that it can be leveraged by different clients. The client is a CLI which users interact with to perform different operations like install/upgrade/delete etc. The client interacts with the Kubernetes API server and the chart repository. It renders Helm template files into Kubernetes manifest files which it uses to perform operations on the Kubernetes cluster via the Kubernetes API. See the Helm Architecture for more details. A chart is organized as a collection of files inside of a directory where the directory name is the name of the chart. It contains template YAML files which facilitates providing configuration values at runtime and eliminates the need of modifying YAML files. These templates provide programming logic as they are based on the Go template language , functions from the Sprig lib and other specialized functions . The chart repository is a location where packaged charts can be stored and shared. This is akin to the image repository in Docker. Refer to The Chart Repository Guide for more details.","title":"Helm Overview"},{"location":"generatedContent/helm101/#helm-abstractions","text":"Helm terms: Chart - It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. A chart is basically a package of pre-configured Kubernetes resources. Config - Contains configuration information that can be merged into a packaged chart to create a releasable object. helm - Helm client. It renders charts into manifest files. It interacts directly with the Kubernetes API server to install, upgrade, query, and remove Kubernetes resources. Release - An instance of a chart running in a Kubernetes cluster. Repository - Place where charts reside and can be shared with others. To get started, head on over to Lab 1 .","title":"Helm Abstractions"},{"location":"generatedContent/helm101/SUMMARY/","text":"Summary \u00b6 Workshop \u00b6 Lab 0 Lab 1 Lab 2 Lab 3 Lab 4 Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/helm101/SUMMARY/#workshop","text":"Lab 0 Lab 1 Lab 2 Lab 3 Lab 4","title":"Workshop"},{"location":"generatedContent/helm101/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/helm101/Lab0/","text":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service \u00b6 The Helm client ( helm ) can be installed from source or pre-built binary releases. In this lab, we are going to use the pre-built binary release (Linux amd64) from the Helm community. Refer to the Helm install docs for more details. Prerequisites \u00b6 Create a Kubernetes cluster with IBM Cloud Kubernetes Service , following the steps to also configure the IBM Cloud CLI with the Kubernetes Service plug-in. Installing the Helm Client (helm) \u00b6 Download the latest release of Helm v3 for your environment, the steps below are for Linux amd64 , adjust the examples as needed for your environment. Unpack it: $ tar -zxvf helm-v3.<x>.<y>-linux-amd64.tgz . Find the helm binary in the unpacked directory, and move it to its desired location: mv linux-amd64/helm /usr/local/bin/helm . It is best if the location you copy to is pathed, as it avoids having to path the helm commands. The Helm client is now installed and can be tested with the command, helm help . Conclusion \u00b6 You are now ready to start using Helm.","title":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service"},{"location":"generatedContent/helm101/Lab0/#lab-0-installing-helm-on-ibm-cloud-kubernetes-service","text":"The Helm client ( helm ) can be installed from source or pre-built binary releases. In this lab, we are going to use the pre-built binary release (Linux amd64) from the Helm community. Refer to the Helm install docs for more details.","title":"Lab 0. Installing Helm on IBM Cloud Kubernetes Service"},{"location":"generatedContent/helm101/Lab0/#prerequisites","text":"Create a Kubernetes cluster with IBM Cloud Kubernetes Service , following the steps to also configure the IBM Cloud CLI with the Kubernetes Service plug-in.","title":"Prerequisites"},{"location":"generatedContent/helm101/Lab0/#installing-the-helm-client-helm","text":"Download the latest release of Helm v3 for your environment, the steps below are for Linux amd64 , adjust the examples as needed for your environment. Unpack it: $ tar -zxvf helm-v3.<x>.<y>-linux-amd64.tgz . Find the helm binary in the unpacked directory, and move it to its desired location: mv linux-amd64/helm /usr/local/bin/helm . It is best if the location you copy to is pathed, as it avoids having to path the helm commands. The Helm client is now installed and can be tested with the command, helm help .","title":"Installing the Helm Client (helm)"},{"location":"generatedContent/helm101/Lab0/#conclusion","text":"You are now ready to start using Helm.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab1/","text":"Lab 1. Deploy with Helm \u00b6 Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application. Scenario 1: Deploy the application using kubectl \u00b6 In this part of the lab, we will deploy the application using the Kubernetes client kubectl . We will use Version 1 of the app for deploying here. If you already have a copy of the guestbook application installed from the kube101 lab , skip this section and go the helm example in Scenario 2 . Clone the Guestbook App repo to get the files: git clone https://github.com/IBM/guestbook.git Use the configuration files in the cloned Git repository to deploy the containers and create services for them by using the following commands: $ cd guestbook/v1 $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created $ kubectl create -f redis-master-service.yaml service/redis-master created $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created $ kubectl create -f redis-slave-service.yaml service/redis-slave created $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created $ kubectl create -f guestbook-service.yaml service/guestbook created Refer to the guestbook README for more details. View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and port of the load balancer in the EXTERNAL-IP and PORTS columns of the $ kubectl get services output. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.252.107 50.23.5.136 3000:31838/TCP 14m redis-master ClusterIP 172.21.97.222 <none> 6379/TCP 14m redis-slave ClusterIP 172.21.43.70 <none> 6379/TCP 14m ......... In this scenario the URL is http://50.23.5.136:31838 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31838 . Navigate to the output given (for example http://50.23.5.136:31838 ) in your browser. You should see the guestbook now displaying in your browser: Scenario 2: Deploy the application using Helm \u00b6 In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo to distinguish it from the previous deployment. The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/IBM/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: kubectl create namespace helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ --namespace helm-demo NAME: guestbook-demo ... You should see output similar to the following: NAME: guestbook-demo LAST DEPLOYED: Mon Feb 24 18:08:02 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: kubectl get deployment guestbook-demo --namespace helm-demo You should see output similar to the following: $ kubectl get deployment guestbook-demo --namespace helm-dem NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: kubectl get pods --namespace helm-demo You should see output similar to the following: $ kubectl get pods --namespace helm-demo NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: kubectl get services --namespace helm-demo $ kubectl get services --namespace helm-demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31367/TCP 52m redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 52m redis-slave ClusterIP 172.21.176.148 <none> 6379/TCP 52m View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: $ export SERVICE_IP = $( kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) $ echo http:// $SERVICE_IP http://50.23.5.136 Combine the service IP with the port of the service printed earlier. In this scenario the URL is http://50.23.5.136:31367 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31367 . Navigate to the output given (for example http://50.23.5.136:31367 ) in your browser. You should see the guestbook now displaying in your browser: Conclusion \u00b6 Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#lab-1-deploy-with-helm","text":"Let's investigate how Helm can help us focus on other things by letting a chart do the work for us. We'll first deploy an application to a Kubernetes cluster by using kubectl and then show how we can offload the work to a chart by deploying the same app with Helm. The application is the Guestbook App , which is a sample multi-tier web application.","title":"Lab 1. Deploy with Helm"},{"location":"generatedContent/helm101/Lab1/#scenario-1-deploy-the-application-using-kubectl","text":"In this part of the lab, we will deploy the application using the Kubernetes client kubectl . We will use Version 1 of the app for deploying here. If you already have a copy of the guestbook application installed from the kube101 lab , skip this section and go the helm example in Scenario 2 . Clone the Guestbook App repo to get the files: git clone https://github.com/IBM/guestbook.git Use the configuration files in the cloned Git repository to deploy the containers and create services for them by using the following commands: $ cd guestbook/v1 $ kubectl create -f redis-master-deployment.yaml deployment.apps/redis-master created $ kubectl create -f redis-master-service.yaml service/redis-master created $ kubectl create -f redis-slave-deployment.yaml deployment.apps/redis-slave created $ kubectl create -f redis-slave-service.yaml service/redis-slave created $ kubectl create -f guestbook-deployment.yaml deployment.apps/guestbook-v1 created $ kubectl create -f guestbook-service.yaml service/guestbook created Refer to the guestbook README for more details. View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and port of the load balancer in the EXTERNAL-IP and PORTS columns of the $ kubectl get services output. $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook LoadBalancer 172.21.252.107 50.23.5.136 3000:31838/TCP 14m redis-master ClusterIP 172.21.97.222 <none> 6379/TCP 14m redis-slave ClusterIP 172.21.43.70 <none> 6379/TCP 14m ......... In this scenario the URL is http://50.23.5.136:31838 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31838 . Navigate to the output given (for example http://50.23.5.136:31838 ) in your browser. You should see the guestbook now displaying in your browser:","title":"Scenario 1: Deploy the application using kubectl"},{"location":"generatedContent/helm101/Lab1/#scenario-2-deploy-the-application-using-helm","text":"In this part of the lab, we will deploy the application by using Helm. We will set a release name of guestbook-demo to distinguish it from the previous deployment. The Helm chart is available here . Clone the Helm 101 repo to get the files: git clone https://github.com/IBM/helm101 A chart is defined as a collection of files that describe a related set of Kubernetes resources. We probably then should take a look at the the files before we go and install the chart. The files for the guestbook chart are as follows: . \u251c\u2500\u2500 Chart.yaml \\\\ A YAML file containing information about the chart \u251c\u2500\u2500 LICENSE \\\\ A plain text file containing the license for the chart \u251c\u2500\u2500 README.md \\\\ A README providing information about the chart usage, configuration, installation etc. \u251c\u2500\u2500 templates \\\\ A directory of templates that will generate valid Kubernetes manifest files when combined with values.yaml \u2502 \u251c\u2500\u2500 _helpers.tpl \\\\ Template helpers/definitions that are re-used throughout the chart \u2502 \u251c\u2500\u2500 guestbook-deployment.yaml \\\\ Guestbook app container resource \u2502 \u251c\u2500\u2500 guestbook-service.yaml \\\\ Guestbook app service resource \u2502 \u251c\u2500\u2500 NOTES.txt \\\\ A plain text file containing short usage notes about how to access the app post install \u2502 \u251c\u2500\u2500 redis-master-deployment.yaml \\\\ Redis master container resource \u2502 \u251c\u2500\u2500 redis-master-service.yaml \\\\ Redis master service resource \u2502 \u251c\u2500\u2500 redis-slave-deployment.yaml \\\\ Redis slave container resource \u2502 \u2514\u2500\u2500 redis-slave-service.yaml \\\\ Redis slave service resource \u2514\u2500\u2500 values.yaml \\\\ The default configuration values for the chart Note: The template files shown above will be rendered into Kubernetes manifest files before being passed to the Kubernetes API server. Therefore, they map to the manifest files that we deployed when we used kubectl (minus the helper and notes files). Let's go ahead and install the chart now. If the helm-demo namespace does not exist, you will need to create it using: kubectl create namespace helm-demo Install the app as a Helm chart: $ cd helm101/charts $ helm install guestbook-demo ./guestbook/ --namespace helm-demo NAME: guestbook-demo ... You should see output similar to the following: NAME: guestbook-demo LAST DEPLOYED: Mon Feb 24 18:08:02 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace helm-demo' export SERVICE_IP=$(kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 The chart install performs the Kubernetes deployments and service creations of the redis master and slaves, and the guestbook app, as one. This is because the chart is a collection of files that describe a related set of Kubernetes resources and Helm manages the creation of these resources via the Kubernetes API. Check the deployment: kubectl get deployment guestbook-demo --namespace helm-demo You should see output similar to the following: $ kubectl get deployment guestbook-demo --namespace helm-dem NAME READY UP-TO-DATE AVAILABLE AGE guestbook-demo 2/2 2 2 51m To check the status of the running application pods, use: kubectl get pods --namespace helm-demo You should see output similar to the following: $ kubectl get pods --namespace helm-demo NAME READY STATUS RESTARTS AGE guestbook-demo-6c9cf8b9-jwbs9 1/1 Running 0 52m guestbook-demo-6c9cf8b9-qk4fb 1/1 Running 0 52m redis-master-5d8b66464f-j72jf 1/1 Running 0 52m redis-slave-586b4c847c-2xt99 1/1 Running 0 52m redis-slave-586b4c847c-q7rq5 1/1 Running 0 52m To check the services, use: kubectl get services --namespace helm-demo $ kubectl get services --namespace helm-demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31367/TCP 52m redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 52m redis-slave ClusterIP 172.21.176.148 <none> 6379/TCP 52m View the guestbook: You can now play with the guestbook that you just created by opening it in a browser (it might take a few moments for the guestbook to come up). Local Host: If you are running Kubernetes locally, view the guestbook by navigating to http://localhost:3000 in your browser. Remote Host: To view the guestbook on a remote host, locate the external IP and the port of the load balancer by following the \"NOTES\" section in the install output. The commands will be similar to the following: $ export SERVICE_IP = $( kubectl get svc --namespace helm-demo guestbook-demo -o jsonpath = '{.status.loadBalancer.ingress[0].ip}' ) $ echo http:// $SERVICE_IP http://50.23.5.136 Combine the service IP with the port of the service printed earlier. In this scenario the URL is http://50.23.5.136:31367 . Note: If no external IP is assigned, then you can get the external IP with the following command: $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.47.122.98 Ready <none> 1h v1.10.11+IKS 173.193.92.112 Ubuntu 16.04.5 LTS 4.4.0-141-generic docker://18.6.1 In this scenario the URL is http://173.193.92.112:31367 . Navigate to the output given (for example http://50.23.5.136:31367 ) in your browser. You should see the guestbook now displaying in your browser:","title":"Scenario 2: Deploy the application using Helm"},{"location":"generatedContent/helm101/Lab1/#conclusion","text":"Congratulations, you have now deployed an application by using two different methods to Kubernetes! From this lab, you can see that using Helm required less commands and less to think about (by giving it the chart path and not the individual files) versus using kubectl . Helm's application management provides the user with this simplicity. Move on to the next lab, Lab2 , to learn how to update our running app when the chart has been changed.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab2/","text":"Lab 2. Make changes with Helm \u00b6 In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . You probably think that you're done and know enough to use Helm. But what about updates or improvements to the chart? How do you update your running app to pick up these changes? In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Removing the Redis slaves and using just the in-memory DB Changing the type from LoadBalancer to NodePort . It seems contrived but the goal of this lab is to show you how to update your apps with Kubernetes and Helm. So, how easy is it to do this? Let's take a look below. Scenario 1: Update the application using kubectl \u00b6 In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. This is an optional step that is not technically required to update your running app. The reason for doing this step is \"house keeping\" - you want to have the correct files for the current configuration that you have deployed. This avoids making mistakes if you have future updates or even rollbacks. In this updated configuration, we remove the Redis slaves. To have the directory match the configuration, move/archive or simply remove the Redis slave files from the guestbook repo tree: cd guestbook/v1 rm redis-slave-service.yaml rm redis-slave-deployment.yaml Note: you can reclaim these files later with a git checkout -- <filename> command, if desired Delete the Redis slave service and pods: $ kubectl delete svc redis-slave --namespace default service \"redis-slave\" deleted $ kubectl delete deployment redis-slave --namespace default deployment.extensions \"redis-slave\" deleted Update the guestbook service from LoadBalancer to NodePort type: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: kubectl delete svc guestbook --namespace default Re-create the service with NodePort type: kubectl create -f guestbook-service.yaml Check the updates, using kubectl get all --namespace default $ kubectl get all --namespace default NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier. Scenario 2: Update the application using Helm \u00b6 In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of this is in redis-slave-service , which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 14:23:27 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, removes the redis-slave resources, and updates the app service.type to NodePort . Check the updates, using kubectl get all --namespace helm-demo : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. All redis-slave resources have been removed. When you check the Helm release with helm list -n helm-demo , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier. Conclusion \u00b6 Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#lab-2-make-changes-with-helm","text":"In Lab 1 , we installed the guestbook sample app by using Helm and saw the benefits over using kubectl . You probably think that you're done and know enough to use Helm. But what about updates or improvements to the chart? How do you update your running app to pick up these changes? In this lab, we're going to look at how to update our running app when the chart has been changed. To demonstrate this, we're going to make changes to the original guestbook chart by: Removing the Redis slaves and using just the in-memory DB Changing the type from LoadBalancer to NodePort . It seems contrived but the goal of this lab is to show you how to update your apps with Kubernetes and Helm. So, how easy is it to do this? Let's take a look below.","title":"Lab 2. Make changes with Helm"},{"location":"generatedContent/helm101/Lab2/#scenario-1-update-the-application-using-kubectl","text":"In this part of the lab we will update the previously deployed application Guestbook , using Kubernetes directly. This is an optional step that is not technically required to update your running app. The reason for doing this step is \"house keeping\" - you want to have the correct files for the current configuration that you have deployed. This avoids making mistakes if you have future updates or even rollbacks. In this updated configuration, we remove the Redis slaves. To have the directory match the configuration, move/archive or simply remove the Redis slave files from the guestbook repo tree: cd guestbook/v1 rm redis-slave-service.yaml rm redis-slave-deployment.yaml Note: you can reclaim these files later with a git checkout -- <filename> command, if desired Delete the Redis slave service and pods: $ kubectl delete svc redis-slave --namespace default service \"redis-slave\" deleted $ kubectl delete deployment redis-slave --namespace default deployment.extensions \"redis-slave\" deleted Update the guestbook service from LoadBalancer to NodePort type: sed -i.bak 's/LoadBalancer/NodePort/g' guestbook-service.yaml Note: you can reset the files later with a git checkout -- <filename> command, if desired Delete the guestbook service: kubectl delete svc guestbook --namespace default Re-create the service with NodePort type: kubectl create -f guestbook-service.yaml Check the updates, using kubectl get all --namespace default $ kubectl get all --namespace default NAME READY STATUS RESTARTS AGE pod/guestbook-v1-7fc76dc46-9r4s7 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-hspnk 1/1 Running 0 1h pod/guestbook-v1-7fc76dc46-sxzkt 1/1 Running 0 1h pod/redis-master-5d8b66464f-pvbl9 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook NodePort 172.21.45.29 <none> 3000:31989/TCP 31s service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 9d service/redis-master ClusterIP 172.21.232.61 <none> 6379/TCP 1h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 3/3 3 3 1h deployment.apps/redis-master 1/1 1 1 1h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-v1-7fc76dc46 3 3 3 1h replicaset.apps/redis-master-5d8b66464f 1 1 1 1h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31989 in this output case) to the guestbook service. All redis-slave resources have been removed. View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 1: Update the application using kubectl"},{"location":"generatedContent/helm101/Lab2/#scenario-2-update-the-application-using-helm","text":"In this section, we'll update the previously deployed guestbook-demo application by using Helm. Before we start, let's take a few minutes to see how Helm simplifies the process compared to using Kubernetes directly. Helm's use of a template language provides great flexibility and power to chart authors, which removes the complexity to the chart user. In the guestbook example, we'll use the following capabilities of templating: Values: An object that provides access to the values passed into the chart. An example of this is in guestbook-service , which contains the line type: {{ .Values.service.type }} . This line provides the capability to set the service type during an upgrade or install. Control structures: Also called \u201cactions\u201d in template parlance, control structures provide the template author with the ability to control the flow of a template\u2019s generation. An example of this is in redis-slave-service , which contains the line {{- if .Values.redis.slaveEnabled -}} . This line allows us to enable/disable the REDIS master/slave during an upgrade or install. The complete redis-slave-service.yaml file shown below, demonstrates how the file becomes redundant when the slaveEnabled flag is disabled and also how the port value is set. There are more examples of templating functionality in the other chart files. {{ - if .Values.redis.slaveEnabled - }} apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : {{ .Values.redis.port }} targetPort : redis-server selector : app : redis role : slave {{ - end }} Enough talking about the theory. Now let's give it a go! First, lets check the app we deployed in Lab 1 with Helm. This can be done by checking the Helm releases: helm list -n helm-demo Note that we specify the namespace. If not specified, it uses the current namespace context. You should see output similar to the following: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 1 2020-02-24 18:08:02.017401264 +0000 UTC deployed guestbook-0.2.0 The list command provides the list of deployed charts (releases) giving information of chart version, namespace, number of updates (revisions) etc. We now know the release is there from step 1., so we can update the application: $ cd helm101/charts $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! ... A Helm upgrade takes an existing release and upgrades it according to the information you provide. You should see output similar to the following: $ helm upgrade guestbook-demo ./guestbook --set redis.slaveEnabled = false,service.type = NodePort --namespace helm-demo Release \"guestbook-demo\" has been upgraded. Happy Helming! NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 14:23:27 2020 NAMESPACE: helm-demo STATUS: deployed REVISION: 2 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace helm-demo -o jsonpath=\"{.spec.ports[0].nodePort}\" services guestbook-demo) export NODE_IP=$(kubectl get nodes --namespace helm-demo -o jsonpath=\"{.items[0].status.addresses[0].address}\") echo http://$NODE_IP:$NODE_PORT The upgrade command upgrades the app to a specified version of a chart, removes the redis-slave resources, and updates the app service.type to NodePort . Check the updates, using kubectl get all --namespace helm-demo : $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn2 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo NodePort 172.21.43.244 <none> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-6c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h Note: The service type has changed (to NodePort ) and a new port has been allocated ( 31202 in this output case) to the guestbook service. All redis-slave resources have been removed. When you check the Helm release with helm list -n helm-demo , you will see the revision and date has been updated: $ helm list -n helm-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo helm-demo 2 2020-02-25 14:23:27.06732381 +0000 UTC deployed guestbook-0.2.0 View the guestbook Get the public IP of one of your nodes: kubectl get nodes -o wide Navigate to the IP address plus the node port that printed earlier.","title":"Scenario 2: Update the application using Helm"},{"location":"generatedContent/helm101/Lab2/#conclusion","text":"Congratulations, you have now updated the applications! Helm does not require any manual changing of resources and is therefore so much easier to upgrade! All configurations can be set on the fly on the command line or by using override files. This is made possible from when the logic was added to the template files, which enables or disables the capability, depending on the flag set. Check out Lab 3 to get an insight into revision management.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab3/","text":"Lab 3. Keeping track of the deployed application \u00b6 Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback? Scenario 1: Revision management using Kubernetes \u00b6 In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management. Scenario 2: Revision management using Helm \u00b6 In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: kubectl get all --namespace helm-demo $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h pod/redis-slave-586b4c847c-tkfj5 1/1 Running 0 5m15s pod/redis-slave-586b4c847c-xxrdn 1/1 Running 0 5m15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h service/redis-slave ClusterIP 172.21.232.16 <none> 6379/TCP 5m15s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h deployment.apps/redis-slave 2/2 2 2 5m15s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-26c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h replicaset.apps/redis-slave-586b4c847c 2 2 2 5m15s You can see from the output that the app service is the service type of LoadBalancer again and the Redis master/slave deployment has returned. This shows a complete rollback from the upgrade in Lab 2 Conclusion \u00b6 From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#lab-3-keeping-track-of-the-deployed-application","text":"Let's say you deployed different release versions of your application (i.e., you upgraded the running application). How do you keep track of the versions and how can you do a rollback?","title":"Lab 3. Keeping track of the deployed application"},{"location":"generatedContent/helm101/Lab3/#scenario-1-revision-management-using-kubernetes","text":"In this part of the lab, we should illustrate revision management of guestbook by using Kubernetes directly, but we can't. This is because Kubernetes does not provide any support for revision management. The onus is on you to manage your systems and any updates or changes you make. However, we can use Helm to conduct revision management.","title":"Scenario 1: Revision management using Kubernetes"},{"location":"generatedContent/helm101/Lab3/#scenario-2-revision-management-using-helm","text":"In this part of the lab, we illustrate revision management on the deployed application guestbook-demo by using Helm. With Helm, every time an install, upgrade, or rollback happens, the revision number is incremented by 1. The first revision number is always 1. Helm persists release metadata in Secrets (default) or ConfigMaps, stored in the Kubernetes cluster. Every time your release changes, it appends that to the existing data. This provides Helm with the capability to rollback to a previous release. Let's see how this works in practice. Check the number of deployments: helm history guestbook-demo -n helm-demo You should see output similar to the following because we did an upgrade in Lab 2 after the initial install in Lab 1 : $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 deployed guestbook-0.2.0 Upgrade complete Roll back to the previous revision: In this rollback, Helm checks the changes that occured when upgrading from the revision 1 to revision 2. This information enables it to makes the calls to the Kubernetes API server, to update the deployed application as per the initial deployment - in other words with Redis slaves and using a load balancer. Rollback with this command: helm rollback guestbook-demo 1 -n helm-demo $ helm rollback guestbook-demo 1 -n helm-demo Rollback was a success! Happy Helming! Check the history again: helm history guestbook-demo -n helm-demo You should see output similar to the following: $ helm history guestbook-demo -n helm-demo REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 1 Mon Feb 24 18:08:02 2020 superseded guestbook-0.2.0 Install complete 2 Tue Feb 25 14:23:27 2020 superseded guestbook-0.2.0 Upgrade complete 3 Tue Feb 25 14:53:45 2020 deployed guestbook-0.2.0 Rollback to 1 Check the rollback, using: kubectl get all --namespace helm-demo $ kubectl get all --namespace helm-demo NAME READY STATUS RESTARTS AGE pod/guestbook-demo-6c9cf8b9-dhqk9 1/1 Running 0 20h pod/guestbook-demo-6c9cf8b9-zddn 1/1 Running 0 20h pod/redis-master-5d8b66464f-g7sh6 1/1 Running 0 20h pod/redis-slave-586b4c847c-tkfj5 1/1 Running 0 5m15s pod/redis-slave-586b4c847c-xxrdn 1/1 Running 0 5m15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/guestbook-demo LoadBalancer 172.21.43.244 <pending> 3000:31202/TCP 20h service/redis-master ClusterIP 172.21.12.43 <none> 6379/TCP 20h service/redis-slave ClusterIP 172.21.232.16 <none> 6379/TCP 5m15s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/guestbook-demo 2/2 2 2 20h deployment.apps/redis-master 1/1 1 1 20h deployment.apps/redis-slave 2/2 2 2 5m15s NAME DESIRED CURRENT READY AGE replicaset.apps/guestbook-demo-26c9cf8b9 2 2 2 20h replicaset.apps/redis-master-5d8b66464f 1 1 1 20h replicaset.apps/redis-slave-586b4c847c 2 2 2 5m15s You can see from the output that the app service is the service type of LoadBalancer again and the Redis master/slave deployment has returned. This shows a complete rollback from the upgrade in Lab 2","title":"Scenario 2: Revision management using Helm"},{"location":"generatedContent/helm101/Lab3/#conclusion","text":"From this lab, we can say that Helm does revision management well and Kubernetes does not have the capability built in! You might be wondering why we need helm rollback when you could just re-run the helm upgrade from a previous version. And that's a good question. Technically, you should end up with the same resources (with same parameters) deployed. However, the advantage of using helm rollback is that helm manages (ie. remembers) all of the variations/parameters of the previous helm install\\upgrade for you. Doing the rollback via a helm upgrade requires you (and your entire team) to manually track how the command was previously executed. That's not only tedious but very error prone. It is much easier, safer and reliable to let Helm manage all of that for you and all you need to do it tell it which previous version to go back to, and it does the rest. Lab 4 awaits.","title":"Conclusion"},{"location":"generatedContent/helm101/Lab4/","text":"Lab 4. Share Helm Charts \u00b6 A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts. Using charts from a public repository \u00b6 Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add helm101 https://raw.githubusercontent.com/IBM/helm101/master/ Should generate an output as follows: $ helm repo add helm101 https://raw.githubusercontent.com/IBM/helm101/master/ \"helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo helm101 $ helm search repo helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.1 A Helm chart to deploy Guestbook three tier web... Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: kubectl create namespace repo-demo Now install the chart using this command: helm install guestbook-demo helm101/guestbook --namespace repo-demo The output should be similar to the following: $ helm install guestbook-demo helm101/guestbook --namespace repo-demo NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 15:40:17 2020 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list -n repo-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2020-02-25 15:40:17.627745329 +0000 UTC deployed guestbook-0.2.1 Conclusion \u00b6 This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers.","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#lab-4-share-helm-charts","text":"A key aspect of providing an application means sharing with others. Sharing can be direct counsumption (by users or in CI/CD pipelines) or as a dependency for other charts. If people can't find your app then they can't use it. A means of sharing is a chart repository, which is a location where packaged charts can be stored and shared. As the chart repository only applies to Helm, we will just look at the usage and storage of Helm charts.","title":"Lab 4. Share Helm Charts"},{"location":"generatedContent/helm101/Lab4/#using-charts-from-a-public-repository","text":"Helm charts can be available on a remote repository or in a local environment/repository. The remote repositories can be public like Bitnami Charts or IBM Helm Charts , or hosted repositories like on Google Cloud Storage or GitHub. Refer to Helm Chart Repository Guide for more details. You can learn more about the structure of a chart repository by examining the chart index file in this lab. In this part of the lab, we show you how to install the guestbook chart from the Helm101 repo . Check the repositories configured on your system: helm repo list The output should be similar to the following: $ helm repo list Error: no repositories to show Note: Chart repositories are not installed by default with Helm v3. It is expected that you add the repositories for the charts you want to use. The Helm Hub provides a centralized search for publicly available distributed charts. Using the hub you can identify the chart with its hosted repository and then add it to your local respoistory list. The Helm chart repository like Helm v2 is in \"maintenance mode\" and will be deprecated by November 13, 2020. See the project status for more details. Add helm101 repo: helm repo add helm101 https://raw.githubusercontent.com/IBM/helm101/master/ Should generate an output as follows: $ helm repo add helm101 https://raw.githubusercontent.com/IBM/helm101/master/ \"helm101\" has been added to your repositories You can also search your repositories for charts by running the following command: helm search repo helm101 $ helm search repo helm101 NAME CHART VERSION APP VERSION DESCRIPTION helm101/guestbook 0.2.1 A Helm chart to deploy Guestbook three tier web... Install the chart As mentioned we are going to install the guestbook chart from the Helm101 repo . As the repo is added to our local respoitory list we can reference the chart using the repo name/chart name , in other words helm101/guestbook . To see this in action, you will install the application to a new namespace called repo-demo . If the repo-demo namespace does not exist, create it using: kubectl create namespace repo-demo Now install the chart using this command: helm install guestbook-demo helm101/guestbook --namespace repo-demo The output should be similar to the following: $ helm install guestbook-demo helm101/guestbook --namespace repo-demo NAME: guestbook-demo LAST DEPLOYED: Tue Feb 25 15:40:17 2020 NAMESPACE: repo-demo STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running 'kubectl get svc -w guestbook-demo --namespace repo-demo' export SERVICE_IP=$(kubectl get svc --namespace repo-demo guestbook-demo -o jsonpath='{.status.loadBalancer.ingress[0].ip}') echo http://$SERVICE_IP:3000 Check that release deployed as expected as follows: $ helm list -n repo-demo NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION guestbook-demo repo-demo 1 2020-02-25 15:40:17.627745329 +0000 UTC deployed guestbook-0.2.1","title":"Using charts from a public repository"},{"location":"generatedContent/helm101/Lab4/#conclusion","text":"This lab provided you with a brief introduction to the Helm repositories to show how charts can be installed. The ability to share your chart means ease of use to both you and your consumers.","title":"Conclusion"},{"location":"generatedContent/kube101/","text":"IBM Cloud Kubernetes Service Lab \u00b6 An introduction to containers \u00b6 Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers. Objectives \u00b6 This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app Prerequisites \u00b6 A Pay-As-You-Go or Subscription IBM Cloud account Virtual machines \u00b6 Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space. Containers \u00b6 Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources VM vs container \u00b6 Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited. Get set up \u00b6 Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\". Kubernetes and containers: an overview \u00b6 Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started What is Kubernetes? \u00b6 Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster. How was Kubernetes created? \u00b6 Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes . Kubernetes architecture \u00b6 At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully. Kubernetes resource model \u00b6 Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model. Key resources \u00b6 A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store. Kubernetes application deployment workflow \u00b6 User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact. Lab information \u00b6 IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads Lab overview \u00b6 Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"IBM Cloud Kubernetes Service Lab"},{"location":"generatedContent/kube101/#ibm-cloud-kubernetes-service-lab","text":"","title":"IBM Cloud Kubernetes Service Lab"},{"location":"generatedContent/kube101/#an-introduction-to-containers","text":"Hey, are you looking for a containers 101 course? Check out our Docker Essentials . Containers allow you to run securely isolated applications with quotas on system resources. Containers started out as an individual feature delivered with the linux kernel. Docker launched with making containers easy to use and developers quickly latched onto that idea. Containers have also sparked an interest in microservice architecture, a design pattern for developing applications in which complex applications are down into smaller, composable pieces which work together. Watch this video to learn about production uses of containers.","title":"An introduction to containers"},{"location":"generatedContent/kube101/#objectives","text":"This lab is an introduction to using Docker containers on Kubernetes in the IBM Cloud Kubernetes Service. By the end of the course, you'll achieve these objectives: Understand core concepts of Kubernetes Build a Docker image and deploy an application on Kubernetes in the IBM Cloud Kubernetes Service Control application deployments, while minimizing your time with infrastructure management Add AI services to extend your app Secure and monitor your cluster and app","title":"Objectives"},{"location":"generatedContent/kube101/#prerequisites","text":"A Pay-As-You-Go or Subscription IBM Cloud account","title":"Prerequisites"},{"location":"generatedContent/kube101/#virtual-machines","text":"Prior to containers, most infrastructure ran not on bare metal, but atop hypervisors managing multiple virtualized operating systems (OSes). This arrangement allowed isolation of applications from one another on a higher level than that provided by the OS. These virtualized operating systems see what looks like their own exclusive hardware. However, this also means that each of these virtual operating systems are replicating an entire OS, taking up disk space.","title":"Virtual machines"},{"location":"generatedContent/kube101/#containers","text":"Containers provide isolation similar to VMs, except provided by the OS and at the process level. Each container is a process or group of processes run in isolation. Typical containers explicitly run only a single process, as they have no need for the standard system services. What they usually need to do can be provided by system calls to the base OS kernel. The isolation on linux is provided by a feature called 'namespaces'. Each different kind of isolation (IE user, cgroups) is provided by a different namespace. This is a list of some of the namespaces that are commonly used and visible to the user: PID - process IDs USER - user and group IDs UTS - hostname and domain name NS - mount points NET - network devices, stacks, and ports CGROUPS - control limits and monitoring of resources","title":"Containers"},{"location":"generatedContent/kube101/#vm-vs-container","text":"Traditional applications are run on native hardware. A single application does not typically use the full resources of a single machine. We try to run multiple applications on a single machine to avoid wasting resources. We could run multiple copies of the same application, but to provide isolation we use VMs to run multiple application instances (VMs) on the same hardware. These VMs have full operating system stacks which make them relatively large and inefficient due to duplication both at runtime and on disk. Containers allow you to share the host OS. This reduces duplication while still providing the isolation. Containers also allow you to drop unneeded files such as system libraries and binaries to save space and reduce your attack surface. If SSHD or LIBC are not installed, they cannot be exploited.","title":"VM vs container"},{"location":"generatedContent/kube101/#get-set-up","text":"Before we dive into Kubernetes, you need to provision a cluster for your containerized app. Then you won't have to wait for it to be ready for the subsequent labs. You must install the CLIs per https://console.ng.bluemix.net/docs/containers/cs_cli_install.html . If you do not yet have these CLIs and the Kubernetes CLI, do lab 0 before starting the course. If you haven't already, provision a cluster. This can take a few minutes, so let it start first: ibmcloud cs cluster-create --name <name-of-cluster> After creation, before using the cluster, make sure it has completed provisioning and is ready for use. Run ibmcloud cs clusters and make sure that your cluster is in state \"deployed\". Then use ibmcloud cs workers <name-of-cluster> and make sure that all worker nodes are in state \"normal\" with Status \"Ready\".","title":"Get set up"},{"location":"generatedContent/kube101/#kubernetes-and-containers-an-overview","text":"Let's talk about Kubernetes orchestration for containers before we build an application on it. We need to understand the following facts about it: What is Kubernetes, exactly? How was Kubernetes created? Kubernetes architecture Kubernetes resource model Kubernetes at IBM Let's get started","title":"Kubernetes and containers: an overview"},{"location":"generatedContent/kube101/#what-is-kubernetes","text":"Now that we know what containers are, let's define what Kubernetes is. Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Your applications may need many other resources to run such as Volumes, Networks, and Secrets that will help you to do things such as connect to databases, talk to firewalled backends, and secure keys. Kubernetes helps you add these resources into your application. Infrastructure resources needed by applications are managed declaratively. Fast fact: Other orchestration technologies are Mesos and Swarm. The key paradigm of kubernetes is it\u2019s Declarative model. The user provides the \"desired state\" and Kubernetes will do it's best make it happen. If you need 5 instances, you do not start 5 separate instances on your own but rather tell Kubernetes that you need 5 instances and Kubernetes will reconcile the state automatically. Simply at this point you need to know that you declare the state you want and Kubernetes makes that happen. If something goes wrong with one of your instances and it crashes, Kubernetes still knows the desired state and creates a new instances on an available node. Fun to know: Kubernetes goes by many names. Sometimes it is shortened to k8s (losing the internal 8 letters), or kube . The word is rooted in ancient Greek and means \"Helmsman\". A helmsman is the person who steers a ship. We hope you can seen the analogy between directing a ship and the decisions made to orchestrate containers on a cluster.","title":"What is Kubernetes?"},{"location":"generatedContent/kube101/#how-was-kubernetes-created","text":"Google wanted to open source their knowledge of creating and running the internal tools Borg & Omega. It adopted Open Governance for Kubernetes by starting the Cloud Native Computing Foundation (CNCF) and giving Kubernetes to that foundation, therefore making it less influenced by Google directly. Many companies such as RedHat, Microsoft, IBM and Amazon quickly joined the foundation. Main entry point for the kubernetes project is at http://kubernetes.io and the source code can be found at https://github.com/kubernetes .","title":"How was Kubernetes created?"},{"location":"generatedContent/kube101/#kubernetes-architecture","text":"At its core, Kubernetes is a data store (etcd). The declarative model is stored in the data store as objects, that means when you say I want 5 instances of a container then that request is stored into the data store. This information change is watched and delegated to Controllers to take action. Controllers then react to the model and attempt to take action to achieve the desired state. The power of Kubernetes is in its simplistic model. As shown, API server is a simple HTTP server handling create/read/update/delete(CRUD) operations on the data store. Then the controller picks up the change you wanted and makes that happen. Controllers are responsible for instantiating the actual resource represented by any Kubernetes resource. These actual resources are what your application needs to allow it to run successfully.","title":"Kubernetes architecture"},{"location":"generatedContent/kube101/#kubernetes-resource-model","text":"Kubernetes Infrastructure defines a resource for every purpose. Each resource is monitored and processed by a controller. When you define your application, it contains a collection of these resources. This collection will then be read by Controllers to build your applications actual backing instances. Some of resources that you may work with are listed below for your reference, for a full list you should go to https://kubernetes.io/docs/concepts/ . In this class we will only use a few of them, like Pod, Deployment, etc. Config Maps holds configuration data for pods to consume. Daemon Sets ensure that each node in the cluster runs this Pod Deployments defines a desired state of a deployment object Events provides lifecycle events on Pods and other deployment objects Endpoints allows a inbound connections to reach the cluster services Ingress is a collection of rules that allow inbound connections to reach the cluster services Jobs creates one or more pods and as they complete successfully the job is marked as completed. Node is a worker machine in Kubernetes Namespaces are multiple virtual clusters backed by the same physical cluster Pods are the smallest deployable units of computing that can be created and managed in Kubernetes Persistent Volumes provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed Replica Sets ensures that a specified number of pod replicas are running at any given time Secrets are intended to hold sensitive information, such as passwords, OAuth tokens, and ssh keys Service Accounts provides an identity for processes that run in a Pod Services is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. Stateful Sets is the workload API object used to manage stateful applications. and more... Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model.","title":"Kubernetes resource model"},{"location":"generatedContent/kube101/#key-resources","text":"A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. A Pod typically represent a process in your cluster. Pods contain at least one container that runs the job and additionally may have other containers in it called sidecars for monitoring, logging, etc. Essentially a Pod is a group of containers. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application. We will see why separating the application logic and backend database into separate Pods will scale better when we build an application shortly. Services define how to expose your app as a DNS entry to have a stable reference. We use query based selector to choose which pods are supplying that service. The user directly manipulates resources via yaml: kubectl ( create | get | apply | delete ) -f myResource.yaml Kubernetes provides us with a client interface through \u2018kubectl\u2019. Kubectl commands allow you to manage your applications, manage cluster and cluster resources, by modifying the model in the data store.","title":"Key resources"},{"location":"generatedContent/kube101/#kubernetes-application-deployment-workflow","text":"User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods - including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact.","title":"Kubernetes application deployment workflow"},{"location":"generatedContent/kube101/#lab-information","text":"IBM Cloud provides the capability to run applications in containers on Kubernetes. The IBM Cloud Kubernetes Service runs Kubernetes clusters which deliver the following: Powerful tools Intuitive user experience Built-in security and isolation to enable rapid delivery of secure applications Cloud services including cognitive capabilities from Watson Capability to manage dedicated cluster resources for both stateless applications and stateful workloads","title":"Lab information"},{"location":"generatedContent/kube101/#lab-overview","text":"Lab 0 (Optional): Provides a walkthrough for installing IBM Cloud command-line tools and the Kubernetes CLI. You can skip this lab if you have the IBM Cloud CLI, the container-service plugin, the containers-registry plugin, and the kubectl CLI already installed on your machine. Lab 1 : This lab walks through creating and deploying a simple \"guestbook\" app written in Go as a net/http Server and accessing it. Lab 2 : Builds on lab 1 to expand to a more resilient setup which can survive having containers fail and recover. Lab 2 will also walk through basic services you need to get started with Kubernetes and the IBM Cloud Kubernetes Service Lab 3 : Builds on lab 2 by increasing the capabilities of the deployed Guestbook application. This lab covers basic distributed application design and how kubernetes helps you use standard design practices. Lab 4 : How to enable your application so Kubernetes can automatically monitor and recover your applications with no user intervention. Lab D : Debugging tips and tricks to help you along your Kubernetes journey. This lab is useful reference that does not follow in a specific sequence of the other labs.","title":"Lab overview"},{"location":"generatedContent/kube101/CONTRIBUTING/","text":"Contributing In General \u00b6 Our project welcomes external contributions! If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request to the GitHub repository . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us via an issue. We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in the queue for a long time, or cannot be accepted at all! Proposing new features \u00b6 If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you spending your valuable time working on a feature that the project developers are not willing to accept into the code base. Fixing bugs \u00b6 If you would like to fix a bug, please raise an issue before sending a pull request so it can be discussed. If the fix is trivial or non controversial then this is not usually necessary. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. Note that if your initial push does not pass TravisCI your change will not be approved. For more details, see the MAINTAINERS page.","title":"Contributing In General"},{"location":"generatedContent/kube101/CONTRIBUTING/#contributing-in-general","text":"Our project welcomes external contributions! If you have an itch, please feel free to scratch it. To contribute code or documentation, please submit a pull request to the GitHub repository . A good way to familiarize yourself with the codebase and contribution process is to look for and tackle low-hanging fruit in the issue tracker . Before embarking on a more ambitious contribution, please quickly get in touch with us via an issue. We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in the queue for a long time, or cannot be accepted at all!","title":"Contributing In General"},{"location":"generatedContent/kube101/CONTRIBUTING/#proposing-new-features","text":"If you would like to implement a new feature, please raise an issue before sending a pull request so the feature can be discussed. This is to avoid you spending your valuable time working on a feature that the project developers are not willing to accept into the code base.","title":"Proposing new features"},{"location":"generatedContent/kube101/CONTRIBUTING/#fixing-bugs","text":"If you would like to fix a bug, please raise an issue before sending a pull request so it can be discussed. If the fix is trivial or non controversial then this is not usually necessary.","title":"Fixing bugs"},{"location":"generatedContent/kube101/CONTRIBUTING/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected. Note that if your initial push does not pass TravisCI your change will not be approved. For more details, see the MAINTAINERS page.","title":"Merge approval"},{"location":"generatedContent/kube101/MAINTAINERS/","text":"Maintainers Guide \u00b6 This guide is intended for maintainers - anybody with commit access to one or more Developer Technology repositories. Maintainers \u00b6 Name GitHub email Nathan Fritze nfritze nfritz@us.ibm.com Nathan LeViere nathanleviere njlevier@gmail.com Methodoology \u00b6 A master branch. This branch MUST be releasable at all times. Commits and merges against this branch MUST contain only bugfixes and/or security fixes. Maintenance releases are tagged against master. A develop branch. This branch contains your proposed changes The remainder of this document details how to merge pull requests to the repositories. Merge approval \u00b6 The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from one of the maintainers of each component affected. Reviewing Pull Requests \u00b6 We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points: Does the change have impact? \u00b6 While fixing typos is nice as it adds to the overall quality of the project, merging a typo fix at a time can be a waste of effort. (Merging many typo fixes because somebody reviewed the entire component, however, is useful!) Other examples to be wary of: Changes in variable names. Ask whether or not the change will make understanding the code easier, or if it could simply a personal preference on the part of the author. Essentially: feel free to close issues that do not have impact. Do the changes make sense? \u00b6 If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the issue for them, and feel free to close the issue once they confirm. Is this a new feature? If so \u00b6 Does the issue contain narrative indicating the need for the feature? If not, ask them to provide that information. Since the issue will be linked in the changelog, this will often be a user's first introduction to it. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction amongst other users, and suggest they re-submit when they can show such support.","title":"Maintainers Guide"},{"location":"generatedContent/kube101/MAINTAINERS/#maintainers-guide","text":"This guide is intended for maintainers - anybody with commit access to one or more Developer Technology repositories.","title":"Maintainers Guide"},{"location":"generatedContent/kube101/MAINTAINERS/#maintainers","text":"Name GitHub email Nathan Fritze nfritze nfritz@us.ibm.com Nathan LeViere nathanleviere njlevier@gmail.com","title":"Maintainers"},{"location":"generatedContent/kube101/MAINTAINERS/#methodoology","text":"A master branch. This branch MUST be releasable at all times. Commits and merges against this branch MUST contain only bugfixes and/or security fixes. Maintenance releases are tagged against master. A develop branch. This branch contains your proposed changes The remainder of this document details how to merge pull requests to the repositories.","title":"Methodoology"},{"location":"generatedContent/kube101/MAINTAINERS/#merge-approval","text":"The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from one of the maintainers of each component affected.","title":"Merge approval"},{"location":"generatedContent/kube101/MAINTAINERS/#reviewing-pull-requests","text":"We recommend reviewing pull requests directly within GitHub. This allows a public commentary on changes, providing transparency for all users. When providing feedback be civil, courteous, and kind. Disagreement is fine, so long as the discourse is carried out politely. If we see a record of uncivil or abusive comments, we will revoke your commit privileges and invite you to leave the project. During your review, consider the following points:","title":"Reviewing Pull Requests"},{"location":"generatedContent/kube101/MAINTAINERS/#does-the-change-have-impact","text":"While fixing typos is nice as it adds to the overall quality of the project, merging a typo fix at a time can be a waste of effort. (Merging many typo fixes because somebody reviewed the entire component, however, is useful!) Other examples to be wary of: Changes in variable names. Ask whether or not the change will make understanding the code easier, or if it could simply a personal preference on the part of the author. Essentially: feel free to close issues that do not have impact.","title":"Does the change have impact?"},{"location":"generatedContent/kube101/MAINTAINERS/#do-the-changes-make-sense","text":"If you do not understand what the changes are or what they accomplish, ask the author for clarification. Ask the author to add comments and/or clarify test case names to make the intentions clear. At times, such clarification will reveal that the author may not be using the code correctly, or is unaware of features that accommodate their needs. If you feel this is the case, work up a code sample that would address the issue for them, and feel free to close the issue once they confirm.","title":"Do the changes make sense?"},{"location":"generatedContent/kube101/MAINTAINERS/#is-this-a-new-feature-if-so","text":"Does the issue contain narrative indicating the need for the feature? If not, ask them to provide that information. Since the issue will be linked in the changelog, this will often be a user's first introduction to it. Are new unit tests in place that test all new behaviors introduced? If not, do not merge the feature until they are! Is documentation in place for the new feature? (See the documentation guidelines). If not do not merge the feature until it is! Is the feature necessary for general use cases? Try and keep the scope of any given component narrow. If a proposed feature does not fit that scope, recommend to the user that they maintain the feature on their own, and close the request. You may also recommend that they see if the feature gains traction amongst other users, and suggest they re-submit when they can show such support.","title":"Is this a new feature? If so"},{"location":"generatedContent/kube101/SUMMARY/","text":"Summary \u00b6 Getting Started \u00b6 Lab 0: Get the IBM Cloud Container Service Labs \u00b6 Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications Resources \u00b6 IBM Developer","title":"Summary"},{"location":"generatedContent/kube101/SUMMARY/#summary","text":"","title":"Summary"},{"location":"generatedContent/kube101/SUMMARY/#getting-started","text":"Lab 0: Get the IBM Cloud Container Service","title":"Getting Started"},{"location":"generatedContent/kube101/SUMMARY/#labs","text":"Lab 1. Set up and deploy your first application Lab 2: Scale and Update Deployments Lab 3: Scale and update apps natively, building multi-tier applications","title":"Labs"},{"location":"generatedContent/kube101/SUMMARY/#resources","text":"IBM Developer","title":"Resources"},{"location":"generatedContent/kube101/Lab0/","text":"Lab 0. Access a Kubernetes cluster \u00b6 Set up your kubernetes environment \u00b6 For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below. Use the IBM Cloud Kubernetes Service \u00b6 You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster. Use a hosted trial environment \u00b6 There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Kubernetes playground on Katacoda This environment starts with a master and worker node pre-configured. You can run the steps from Labs 1 and onward from the master node. Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers. Set up on your own workstation \u00b6 If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image. Install the IBM Cloud command-line interface \u00b6 As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option. Install the IBM Cloud Kubernetes Service plug-in \u00b6 To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service . Download the Kubernetes CLI \u00b6 To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl Configure Kubectl to point to IBM Cloud Kubernetes Service \u00b6 List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m Download the Workshop Source Code \u00b6 Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Lab 0. Access a Kubernetes cluster"},{"location":"generatedContent/kube101/Lab0/#lab-0-access-a-kubernetes-cluster","text":"","title":"Lab 0. Access a Kubernetes cluster"},{"location":"generatedContent/kube101/Lab0/#set-up-your-kubernetes-environment","text":"For the hands-on labs in this tutorial repository, you will need a kubernetes cluster. One option for creating a cluster is to make use of the Kubernetes as-a-service from the IBM Cloud Kubernetes Service as outlined below.","title":"Set up your kubernetes environment"},{"location":"generatedContent/kube101/Lab0/#use-the-ibm-cloud-kubernetes-service","text":"You will need either a paid IBM Cloud account or an IBM Cloud account which is a Trial account (not a Lite account). If you have one of these accounts, use the Getting Started Guide to create your cluster.","title":"Use the IBM Cloud Kubernetes Service"},{"location":"generatedContent/kube101/Lab0/#use-a-hosted-trial-environment","text":"There are a few services that are accessible over the Internet for temporary use. As these are free services, they can sometimes experience periods of limited availablity/quality. On the other hand, they can be a quick way to get started! Kubernetes playground on Katacoda This environment starts with a master and worker node pre-configured. You can run the steps from Labs 1 and onward from the master node. Play with Kubernetes After signing in with your github or docker hub id, click on Start , then Add New Instance and follow steps shown in terminal to spin up the cluster and add workers.","title":"Use a hosted trial environment"},{"location":"generatedContent/kube101/Lab0/#set-up-on-your-own-workstation","text":"If you would like to configure kubernetes to run on your local workstation for non-production, learning use, there are several options. Minikube This solution requires the installation of a supported VM provider (KVM, VirtualBox, HyperKit, Hyper-V - depending on platform) Kubernetes in Docker (kind) Runs a kubernetes cluster on Docker containers Docker Desktop (Mac) Docker Desktop (Windows) Docker Desktop includes a kubernetes environment Microk8s Installable kubernetes packaged as an Ubuntu snap image.","title":"Set up on your own workstation"},{"location":"generatedContent/kube101/Lab0/#install-the-ibm-cloud-command-line-interface","text":"As a prerequisite for the IBM Cloud Kubernetes Service plug-in, install the IBM Cloud command-line interface . Once installed, you can access IBM Cloud from your command-line with the prefix bx . Log in to the IBM Cloud CLI: ibmcloud login . Enter your IBM Cloud credentials when prompted. Note: If you have a federated ID, use ibmcloud login --sso to log in to the IBM Cloud CLI. Enter your user name, and use the provided URL in your CLI output to retrieve your one-time passcode. You know you have a federated ID when the login fails without the --sso and succeeds with the --sso option.","title":"Install the IBM Cloud command-line interface"},{"location":"generatedContent/kube101/Lab0/#install-the-ibm-cloud-kubernetes-service-plug-in","text":"To create Kubernetes clusters and manage worker nodes, install the IBM Cloud Kubernetes Service plug-in: ibmcloud plugin install container-service -r Bluemix Note: The prefix for running commands by using the IBM Cloud Kubernetes Service plug-in is bx cs . To verify that the plug-in is installed properly, run the following command: ibmcloud plugin list The IBM Cloud Kubernetes Service plug-in is displayed in the results as container-service .","title":"Install the IBM Cloud Kubernetes Service plug-in"},{"location":"generatedContent/kube101/Lab0/#download-the-kubernetes-cli","text":"To view a local version of the Kubernetes dashboard and to deploy apps into your clusters, you will need to install the Kubernetes CLI that corresponds with your operating system: OS X Linux Windows For Windows users: Install the Kubernetes CLI in the same directory as the IBM Cloud CLI. This setup saves you some filepath changes when you run commands later. For OS X and Linux users: Move the executable file to the /usr/local/bin directory using the command mv /<path_to_file>/kubectl /usr/local/bin/kubectl . Make sure that /usr/local/bin is listed in your PATH system variable. $ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin Convert the binary file to an executable: chmod +x /usr/local/bin/kubectl","title":"Download the Kubernetes CLI"},{"location":"generatedContent/kube101/Lab0/#configure-kubectl-to-point-to-ibm-cloud-kubernetes-service","text":"List the clusters in your account: ibmcloud ks clusters Set an environment variable that will be used in subsequent commands in this lab. export CLUSTER_NAME = <your_cluster_name> Configure kubectl to point to your cluster ibmcloud ks cluster config --cluster $CLUSTER_NAME Validate proper configuration kubectl get namespace You should see output similar to the following, if so, then your're ready to continue. NAME STATUS AGE default Active 125m ibm-cert-store Active 121m ibm-system Active 124m kube-node-lease Active 125m kube-public Active 125m kube-system Active 125m","title":"Configure Kubectl to point to IBM Cloud Kubernetes Service"},{"location":"generatedContent/kube101/Lab0/#download-the-workshop-source-code","text":"Repo guestbook has the application that we'll be deploying. While we're not going to build it we will use the deployment configuration files from that repo. Guestbook application has two versions v1 and v2 which we will use to demonstrate some rollout functionality later. All the configuration files we use are under the directory guestbook/v1. Repo kube101 contains the step by step instructions to run the workshop. git clone https://github.com/IBM/guestbook.git git clone https://github.com/IBM/kube101.git","title":"Download the Workshop Source Code"},{"location":"generatedContent/kube101/Lab1/","text":"Lab 1. Deploy your first application \u00b6 Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service. 1. Deploy the guestbook application \u00b6 In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <public-IP> is 169.59.228.215 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 169.59.228.215:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#lab-1-deploy-your-first-application","text":"Learn how to deploy an application to a Kubernetes cluster hosted within the IBM Container Service.","title":"Lab 1. Deploy your first application"},{"location":"generatedContent/kube101/Lab1/#1-deploy-the-guestbook-application","text":"In this part of the lab we will deploy an application called guestbook that has already been built and uploaded to DockerHub under the name ibmcom/guestbook:v1 . Start by running guestbook : kubectl create deployment guestbook --image = ibmcom/guestbook:v1 This action will take a bit of time. To check the status of the running application, you can use $ kubectl get pods . You should see output similar to the following: kubectl get pods Eventually, the status should show up as Running . $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-59bd679fdc-bxdg7 1 /1 Running 0 1m The end result of the run command is not just the pod containing our application containers, but a Deployment resource that manages the lifecycle of those pods. Once the status reads Running , we need to expose that deployment as a service so we can access it through the IP of the worker nodes. The guestbook application listens on port 3000. Run: kubectl expose deployment guestbook --type = \"NodePort\" --port = 3000 To find the port used on that worker node, examine your new service: $ kubectl get service guestbook NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook NodePort 10 .10.10.253 <none> 3000 :31208/TCP 1m We can see that our <nodeport> is 31208 . We can see in the output the port mapping from 3000 inside the pod exposed to the cluster on port 31208. This port in the 31000 range is automatically chosen, and could be different for you. guestbook is now running on your cluster, and exposed to the internet. We need to find out where it is accessible. The worker nodes running in the container service get external IP addresses. Get the workers for your cluster and note one (any one) of the public IPs listed on the <public-IP> line. Replace $CLUSTER_NAME with your cluster name unless you have this environment variable set. $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10 .185.199.3 Ready master,worker 63d v1.16.2+283af84 10 .185.199.3 169 .59.228.215 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 10 .185.199.6 Ready master,worker 63d v1.16.2+283af84 10 .185.199.6 169 .47.78.51 Red Hat 3 .10.0-1127.13.1.el7.x86_64 cri-o://1.16.6-17.rhaos4.3.git4936f44.el7 We can see that our <public-IP> is 169.59.228.215 . Now that you have both the address and the port, you can now access the application in the web browser at <public-IP>:<nodeport> . In the example case this is 169.59.228.215:31208 . Congratulations, you've now deployed an application to Kubernetes! When you're all done, continue to the next lab of this course .","title":"1. Deploy the guestbook application"},{"location":"generatedContent/kube101/Lab1/script/script/","text":"Pod \u00b6 In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80 Labels \u00b6 In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m Deployments \u00b6 A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m services \u00b6 Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"Pod"},{"location":"generatedContent/kube101/Lab1/script/script/#pod","text":"In Kubernetes, a group of one or more containers is called a pod. Containers in a pod are deployed together, and are started, stopped, and replicated as a group. The simplest pod definition describes the deployment of a single container. For example, an nginx web server pod might be defined as such: apiVersion : v1 kind : Pod metadata : name : mynginx namespace : default labels : run : nginx spec : containers : - name : mynginx image : nginx:latest ports : - containerPort : 80","title":"Pod"},{"location":"generatedContent/kube101/Lab1/script/script/#labels","text":"In Kubernetes, labels are a system to organize objects into groups. Labels are key-value pairs that are attached to each object. Label selectors can be passed along with a request to the apiserver to retrieve a list of objects which match that label selector. To add a label to a pod, add a labels section under metadata in the pod definition: apiVersion : v1 kind : Pod metadata : labels : run : nginx ... To label a running pod kubectl label pod mynginx type = webserver pod \"mynginx\" labeled To list pods based on labels kubectl get pods -l type = webserver NAME READY STATUS RESTARTS AGE mynginx 1 /1 Running 0 21m","title":"Labels"},{"location":"generatedContent/kube101/Lab1/script/script/#deployments","text":"A Deployment provides declarative updates for pods and replicas. You only need to describe the desired state in a Deployment object, and it will change the actual state to the desired state. The Deployment object defines the following details: The elements of a Replication Controller definition The strategy for transitioning between deployments To create a deployment for a nginx webserver, edit the nginx-deploy.yaml file as apiVersion : apps/v1beta1 kind : Deployment metadata : generation : 1 labels : run : nginx name : nginx namespace : default spec : replicas : 3 selector : matchLabels : run : nginx strategy : rollingUpdate : maxSurge : 1 maxUnavailable : 1 type : RollingUpdate template : metadata : labels : run : nginx spec : containers : - image : nginx:latest imagePullPolicy : Always name : nginx ports : - containerPort : 80 protocol : TCP dnsPolicy : ClusterFirst restartPolicy : Always securityContext : {} terminationGracePeriodSeconds : 30 and create the deployment kubectl create -f nginx-deploy.yaml deployment \"nginx\" created The deployment creates the following objects kubectl get all -l run = nginx NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/nginx 3 3 3 3 4m NAME DESIRED CURRENT READY AGE rs/nginx-664452237 3 3 3 4m NAME READY STATUS RESTARTS AGE po/nginx-664452237-h8dh0 1 /1 Running 0 4m po/nginx-664452237-ncsh1 1 /1 Running 0 4m po/nginx-664452237-vts63 1 /1 Running 0 4m","title":"Deployments"},{"location":"generatedContent/kube101/Lab1/script/script/#services","text":"Services Kubernetes pods, as containers, are ephemeral. Replication Controllers create and destroy pods dynamically, e.g. when scaling up or down or when doing rolling updates. While each pod gets its own IP address, even those IP addresses cannot be relied upon to be stable over time. This leads to a problem: if some set of pods provides functionality to other pods inside the Kubernetes cluster, how do those pods find out and keep track of which other? A Kubernetes Service is an abstraction which defines a logical set of pods and a policy by which to access them. The set of pods targeted by a Service is usually determined by a label selector. Kubernetes offers a simple Endpoints API that is updated whenever the set of pods in a service changes. To create a service for our nginx webserver, edit the nginx-service.yaml file apiVersion : v1 kind : Service metadata : name : nginx labels : run : nginx spec : selector : run : nginx ports : - protocol : TCP port : 8000 targetPort : 80 type : ClusterIP Create the service kubectl create -f nginx-service.yaml service \"nginx\" created kubectl get service -l run = nginx NAME CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE nginx 10 .254.60.24 <none> 8000 /TCP 38s Describe the service: kubectl describe service nginx Name: nginx Namespace: default Labels: run = nginx Selector: run = nginx Type: ClusterIP IP: 10 .254.60.24 Port: <unset> 8000 /TCP Endpoints: 172 .30.21.3:80,172.30.4.4:80,172.30.53.4:80 Session Affinity: None No events. The above service is associated to our previous nginx pods. Pay attention to the service selector run=nginx field. It tells Kubernetes that all pods with the label run=nginx are associated to this service, and should have traffic distributed amongst them. In other words, the service provides an abstraction layer, and it is the input point to reach all of the associated pods.","title":"services"},{"location":"generatedContent/kube101/Lab2/","text":"Lab 2: Scale and Update Deployments \u00b6 In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1 1. Scale apps with replicas \u00b6 A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram: 2. Update and roll back apps \u00b6 Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"Lab 2. Scale and update deployments"},{"location":"generatedContent/kube101/Lab2/#lab-2-scale-and-update-deployments","text":"In this lab, you'll learn how to update the number of instances a deployment has and how to safely roll out an update of your application on Kubernetes. For this lab, you need a running deployment of the guestbook application from the previous lab. If you need to create it, run: kubectl create deployment guestbook --image = ibmcom/guestbook:v1","title":"Lab 2: Scale and Update Deployments"},{"location":"generatedContent/kube101/Lab2/#1-scale-apps-with-replicas","text":"A replica is a copy of a pod that contains a running service. By having multiple replicas of a pod, you can ensure your deployment has the available resources to handle increasing load on your application. kubectl provides a scale subcommand to change the size of an existing deployment. Let's increase our capacity from a single running instance of guestbook up to 10 instances: kubectl scale --replicas = 10 deployment guestbook Kubernetes will now try to make reality match the desired state of 10 replicas by starting 9 new pods with the same configuration as the first. To see your changes being rolled out, you can run: kubectl rollout status deployment guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment guestbook Waiting for rollout to finish: 1 of 10 updated replicas are available... Waiting for rollout to finish: 2 of 10 updated replicas are available... Waiting for rollout to finish: 3 of 10 updated replicas are available... Waiting for rollout to finish: 4 of 10 updated replicas are available... Waiting for rollout to finish: 5 of 10 updated replicas are available... Waiting for rollout to finish: 6 of 10 updated replicas are available... Waiting for rollout to finish: 7 of 10 updated replicas are available... Waiting for rollout to finish: 8 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Once the rollout has finished, ensure your pods are running by using: kubectl get pods You should see output listing 10 replicas of your deployment: $ kubectl get pods NAME READY STATUS RESTARTS AGE guestbook-562211614-1tqm7 1 /1 Running 0 1d guestbook-562211614-1zqn4 1 /1 Running 0 2m guestbook-562211614-5htdz 1 /1 Running 0 2m guestbook-562211614-6h04h 1 /1 Running 0 2m guestbook-562211614-ds9hb 1 /1 Running 0 2m guestbook-562211614-nb5qp 1 /1 Running 0 2m guestbook-562211614-vtfp2 1 /1 Running 0 2m guestbook-562211614-vz5qw 1 /1 Running 0 2m guestbook-562211614-zksw3 1 /1 Running 0 2m guestbook-562211614-zsp0j 1 /1 Running 0 2m Tip: Another way to improve availability is to add clusters and regions to your deployment, as shown in the following diagram:","title":"1. Scale apps with replicas"},{"location":"generatedContent/kube101/Lab2/#2-update-and-roll-back-apps","text":"Kubernetes allows you to do rolling upgrade of your application to a new container image. This allows you to easily update the running image and also allows you to easily undo a rollout if a problem is discovered during or after deployment. In the previous lab, we used an image with a v1 tag. For our upgrade we'll use the image with the v2 tag. To update and roll back: Using kubectl , you can now update your deployment to use the v2 image. kubectl allows you to change details about existing resources with the set subcommand. We can use it to change the image being used. kubectl set image deployment/guestbook guestbook = ibmcom/guestbook:v2 Note that a pod could have multiple containers, each with its own name. Each image can be changed individually or all at once by referring to the name. In the case of our guestbook Deployment, the container name is also guestbook . Multiple containers can be updated at the same time. ( More information .) To check the status of the rollout, run: kubectl rollout status deployment/guestbook The rollout might occur so quickly that the following messages might not display: $ kubectl rollout status deployment/guestbook Waiting for rollout to finish: 2 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 3 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 4 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 5 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 6 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 7 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 8 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 9 out of 10 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... Waiting for rollout to finish: 9 of 10 updated replicas are available... deployment \"guestbook\" successfully rolled out Test the application as before, by accessing <public-IP>:<nodeport> in the browser to confirm your new code is active. Remember, to get the \"nodeport\" and \"public-ip\" use the following commands. Replace $CLUSTER_NAME with the name of your cluster if the environment variable is not set.: kubectl describe service guestbook and kubectl get nodes -o wide To verify that you're running \"v2\" of guestbook, look at the title of the page, it should now be Guestbook - v2 . If you are using a browser, make sure you force refresh (invalidating your cache). If you want to undo your latest rollout, use: kubectl rollout undo deployment guestbook You can then use this command to see the status: kubectl rollout status deployment/guestbook When doing a rollout, you see references to old replicas and new replicas. The old replicas are the original 10 pods deployed when we scaled the application. The new replicas come from the newly created pods with the different image. All of these pods are owned by the Deployment. The deployment manages these two sets of pods with a resource called a ReplicaSet. We can see the guestbook ReplicaSets with: $ kubectl get replicasets -l app = guestbook NAME DESIRED CURRENT READY AGE guestbook-5f5548d4f 10 10 10 21m guestbook-768cc55c78 0 0 0 3h Before we continue, let's delete the application so we can learn about a different way to achieve the same results: To remove the deployment, use kubectl delete deployment guestbook To remove the service, use: kubectl delete service guestbook Congratulations! You deployed the second version of the app. Lab 2 is now complete. Continue to the next lab of this course .","title":"2. Update and roll back apps"},{"location":"generatedContent/kube101/Lab3/","text":"Lab 3: Scale and update apps natively, building multi-tier applications \u00b6 In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory. 1. Scale apps natively \u00b6 Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-external-node-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide 2. Connect to a back-end service \u00b6 If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-external-node-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! $ kubectl exec -it redis-slave-kd7vx redis-cli 127 .0.0.1:6379> keys * 1 ) \"guestbook\" 127 .0.0.1:6379> lrange guestbook 0 10 1 ) \"hello world\" 2 ) \"welcome to the Kube workshop\" 127 .0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the redis-slave pods while \"write\" operations will go to the redis-master pods. redis-slave-service.yaml apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"Lab 3. Build multi-tier applications"},{"location":"generatedContent/kube101/Lab3/#lab-3-scale-and-update-apps-natively-building-multi-tier-applications","text":"In this lab you'll learn how to deploy the same guestbook application we deployed in the previous labs, however, instead of using the kubectl command line helper functions we'll be deploying the application using configuration files. The configuration file mechanism allows you to have more fine-grained control over all of resources being created within the Kubernetes cluster. Before we work with the application we need to clone a github repo: git clone https://github.com/IBM/guestbook.git This repo contains multiple versions of the guestbook application as well as the configuration files we'll use to deploy the pieces of the application. Change directory by running the command cd guestbook/v1 You will find all the configurations files for this exercise in this directory.","title":"Lab 3: Scale and update apps natively, building multi-tier applications"},{"location":"generatedContent/kube101/Lab3/#1-scale-apps-natively","text":"Kubernetes can deploy an individual pod to run an application but when you need to scale it to handle a large number of requests a Deployment is the resource you want to use. A Deployment manages a collection of similar pods. When you ask for a specific number of replicas the Kubernetes Deployment Controller will attempt to maintain that number of replicas at all times. Every Kubernetes object we create should provide two nested object fields that govern the object\u2019s configuration: the object spec and the object status . Object spec defines the desired state, and object status contains Kubernetes system provided information about the actual state of the resource. As described before, Kubernetes will attempt to reconcile your desired state with the actual state of the system. For Object that we create we need to provide the apiVersion you are using to create the object, kind of the object we are creating and the metadata about the object such as a name , set of labels and optionally namespace that this object should belong. Consider the following deployment configuration for guestbook application guestbook-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : guestbook-v1 labels : app : guestbook version : \"1.0\" spec : replicas : 3 selector : matchLabels : app : guestbook template : metadata : labels : app : guestbook version : \"1.0\" spec : containers : - name : guestbook image : ibmcom/guestbook:v1 ports : - name : http-server containerPort : 3000 The above configuration file create a deployment object named 'guestbook' with a pod containing a single container running the image ibmcom/guestbook:v1 . Also the configuration specifies replicas set to 3 and Kubernetes tries to make sure that at least three active pods are running at all times. Create guestbook deployment To create a Deployment using this configuration file we use the following command: kubectl create -f guestbook-deployment.yaml List the pod with label app=guestbook We can then list the pods it created by listing all pods that have a label of \"app\" with a value of \"guestbook\". This matches the labels defined above in the yaml file in the spec.template.metadata.labels section. kubectl get pods -l app = guestbook When you change the number of replicas in the configuration, Kubernetes will try to add, or remove, pods from the system to match your request. To can make these modifications by using the following command: kubectl edit deployment guestbook-v1 This will retrieve the latest configuration for the Deployment from the Kubernetes server and then load it into an editor for you. You'll notice that there are a lot more fields in this version than the original yaml file we used. This is because it contains all of the properties about the Deployment that Kubernetes knows about, not just the ones we chose to specify when we create it. Also notice that it now contains the status section mentioned previously. To exit the vi editor, type :q! , of if you made changes that you want to see reflected, save them using :wq . You can also edit the deployment file we used to create the Deployment to make changes. You should use the following command to make the change effective when you edit the deployment locally. kubectl apply -f guestbook-deployment.yaml This will ask Kubernetes to \"diff\" our yaml file with the current state of the Deployment and apply just those changes. We can now define a Service object to expose the deployment to external clients. guestbook-service.yaml apiVersion : v1 kind : Service metadata : name : guestbook labels : app : guestbook spec : ports : - port : 3000 targetPort : http-server selector : app : guestbook type : LoadBalancer The above configuration creates a Service resource named guestbook. A Service can be used to create a network path for incoming traffic to your running application. In this case, we are setting up a route from port 3000 on the cluster to the \"http-server\" port on our app, which is port 3000 per the Deployment container spec. Let us now create the guestbook service using the same type of command we used when we created the Deployment: kubectl create -f guestbook-service.yaml Test guestbook app using a browser of your choice using the url <your-external-node-ip>:<node-port> Remember, to get the nodeport and public-ip use the following commands, replacing $CLUSTER_NAME with the name of your cluster if the environment variable is not already set. kubectl describe service guestbook and kubectl get nodes -o wide","title":"1. Scale apps natively"},{"location":"generatedContent/kube101/Lab3/#2-connect-to-a-back-end-service","text":"If you look at the guestbook source code, under the guestbook/v1/guestbook directory, you'll notice that it is written to support a variety of data stores. By default it will keep the log of guestbook entries in memory. That's ok for testing purposes, but as you get into a more \"real\" environment where you scale your application that model will not work because based on which instance of the application the user is routed to they'll see very different results. To solve this we need to have all instances of our app share the same data store - in this case we're going to use a redis database that we deploy to our cluster. This instance of redis will be defined in a similar manner to the guestbook. redis-master-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-master labels : app : redis role : master spec : replicas : 1 selector : matchLabels : app : redis role : master template : metadata : labels : app : redis role : master spec : containers : - name : redis-master image : redis:3.2.9 ports : - name : redis-server containerPort : 6379 This yaml creates a redis database in a Deployment named 'redis-master'. It will create a single instance, with replicas set to 1, and the guestbook app instances will connect to it to persist data, as well as read the persisted data back. The image running in the container is 'redis:3.2.9' and exposes the standard redis port 6379. Create a redis Deployment, like we did for guestbook: kubectl create -f redis-master-deployment.yaml Check to see that redis server pod is running: $ kubectl get pods -lapp = redis,role = master NAME READY STATUS RESTARTS AGE redis-master-q9zg7 1 /1 Running 0 2d Let us test the redis standalone. Replace the pod name redis-master-q9zg7 with the name of your pod. kubectl exec -it redis-master-q9zg7 redis-cli The kubectl exec command will start a secondary process in the specified container. In this case we're asking for the \"redis-cli\" command to be executed in the container named \"redis-master-q9zg7\". When this process ends the \"kubectl exec\" command will also exit but the other processes in the container will not be impacted. Once in the container we can use the \"redis-cli\" command to make sure the redis database is running properly, or to configure it if needed. redis-cli> ping PONG redis-cli> exit Now we need to expose the redis-master Deployment as a Service so that the guestbook application can connect to it through DNS lookup. redis-master-service.yaml apiVersion : v1 kind : Service metadata : name : redis-master labels : app : redis role : master spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : master This creates a Service object named 'redis-master' and configures it to target port 6379 on the pods selected by the selectors \"app=redis\" and \"role=master\". Create the service to access redis master: kubectl create -f redis-master-service.yaml Restart guestbook so that it will find the redis service to use database: kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-external-node-ip>:<node-port> , or by refreshing the page if you already have the app open in another window. You can see now that if you open up multiple browsers and refresh the page to access the different copies of guestbook that they all have a consistent state. All instances write to the same backing persistent storage, and all instances read from that storage to display the guestbook entries that have been stored. We have our simple 3-tier application running but we need to scale the application if traffic increases. Our main bottleneck is that we only have one database server to process each request coming though guestbook. One simple solution is to separate the reads and write such that they go to different databases that are replicated properly to achieve data consistency. Create a deployment named 'redis-slave' that can talk to redis database to manage data reads. In order to scale the database we use the pattern where we can scale the reads using redis slave deployment which can run several instances to read. Redis slave deployments is configured to run two replicas. redis-slave-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : redis-slave labels : app : redis role : slave spec : replicas : 2 selector : matchLabels : app : redis role : slave template : metadata : labels : app : redis role : slave spec : containers : - name : redis-slave image : ibmcom/guestbook-redis-slave:v2 ports : - name : redis-server containerPort : 6379 Create the pod running redis slave deployment. kubectl create -f redis-slave-deployment.yaml Check if all the slave replicas are running $ kubectl get pods -lapp = redis,role = slave NAME READY STATUS RESTARTS AGE redis-slave-kd7vx 1 /1 Running 0 2d redis-slave-wwcxw 1 /1 Running 0 2d And then go into one of those pods and look at the database to see that everything looks right. Replace the pod name redis-slave-kd7vx with your own pod name. If you get the back (empty list or set) when you print the keys, go to the guestbook application and add an entry! $ kubectl exec -it redis-slave-kd7vx redis-cli 127 .0.0.1:6379> keys * 1 ) \"guestbook\" 127 .0.0.1:6379> lrange guestbook 0 10 1 ) \"hello world\" 2 ) \"welcome to the Kube workshop\" 127 .0.0.1:6379> exit Deploy redis slave service so we can access it by DNS name. Once redeployed, the application will send \"read\" operations to the redis-slave pods while \"write\" operations will go to the redis-master pods. redis-slave-service.yaml apiVersion : v1 kind : Service metadata : name : redis-slave labels : app : redis role : slave spec : ports : - port : 6379 targetPort : redis-server selector : app : redis role : slave Create the service to access redis slaves. kubectl create -f redis-slave-service.yaml Restart guestbook so that it will find the slave service to read from. kubectl delete deploy guestbook-v1 kubectl create -f guestbook-deployment.yaml Test guestbook app using a browser of your choice using the url <your-cluster-ip>:<node-port> , or by refreshing the page if you have the app open in another window. That's the end of the lab. Now let's clean-up our environment: kubectl delete -f guestbook-deployment.yaml kubectl delete -f guestbook-service.yaml kubectl delete -f redis-slave-service.yaml kubectl delete -f redis-slave-deployment.yaml kubectl delete -f redis-master-service.yaml kubectl delete -f redis-master-deployment.yaml","title":"2. Connect to a back-end service"},{"location":"generatedContent/kube101/Lab4/","text":"UNDER CONSTRUCTION \u00b6 1. Check the health of apps \u00b6 Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"***UNDER CONSTRUCTION***"},{"location":"generatedContent/kube101/Lab4/#under-construction","text":"","title":"UNDER CONSTRUCTION"},{"location":"generatedContent/kube101/Lab4/#1-check-the-health-of-apps","text":"Kubernetes uses availability checks (liveness probes) to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs. Also, Kubernetes uses readiness checks to know when a container is ready to start accepting traffic. A pod is considered ready when all of its containers are ready. One use of this check is to control which pods are used as backends for services. When a pod is not ready, it is removed from load balancers. In this example, we have defined a HTTP liveness probe to check health of the container every five seconds. For the first 10-15 seconds the /healthz returns a 200 response and will fail afterward. Kubernetes will automatically restart the service. Open the healthcheck.yml file with a text editor. This configuration script combines a few steps from the previous lesson to create a deployment and a service at the same time. App developers can use these scripts when updates are made or to troubleshoot issues by re-creating the pods: Update the details for the image in your private registry namespace: image : \"ibmcom/guestbook:v2\" Note the HTTP liveness probe that checks the health of the container every five seconds. livenessProbe : httpGet : path : /healthz port : 3000 initialDelaySeconds : 5 periodSeconds : 5 In the Service section, note the NodePort . Rather than generating a random NodePort like you did in the previous lesson, you can specify a port in the 30000 - 32767 range. This example uses 30072. Run the configuration script in the cluster. When the deployment and the service are created, the app is available for anyone to see: kubectl apply -f healthcheck.yml Now that all the deployment work is done, check how everything turned out. You might notice that because more instances are running, things might run a bit slower. Open a browser and check out the app. To form the URL, combine the IP with the NodePort that was specified in the configuration script. To get the public IP address for the worker node: ibmcloud cs workers <cluster-name> In a browser, you'll see a success message. If you do not see this text, don't worry. This app is designed to go up and down. For the first 10 - 15 seconds, a 200 message is returned, so you know that the app is running successfully. After those 15 seconds, a timeout message is displayed, as is designed in the app. Launch your Kubernetes dashboard: Get your credentials for Kubernetes. kubectl config view -o jsonpath = '{.users[0].user.auth-provider.config.id-token}' Copy the id-token value that is shown in the output. Set the proxy with the default port number. kubectl proxy Output: Starting to serve on 127 .0.0.1:8001 Sign in to the dashboard. Open the following URL in a web browser. http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ In the sign-on page, select the Token authentication method. Then, paste the id-token value that you previously copied into the Token field and click SIGN IN . In the Workloads tab, you can see the resources that you created. From this tab, you can continually refresh and see that the health check is working. In the Pods section, you can see how many times the pods are restarted when the containers in them are re-created. You might happen to catch errors in the dashboard, indicating that the health check caught a problem. Give it a few minutes and refresh again. You see the number of restarts changes for each pod. Ready to delete what you created before you continue? This time, you can use the same configuration script to delete both of the resources you created. kubectl delete -f healthcheck.yml When you are done exploring the Kubernetes dashboard, in your CLI, enter CTRL+C to exit the proxy command.","title":"1. Check the health of apps"},{"location":"generatedContent/kube101/LabD/","text":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes \u00b6 Advanced debugging techniques to reach your pods. Pod Logs \u00b6 You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached. kubectl edit and vi \u00b6 By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file. busybox pod \u00b6 For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests. Service Endpoints \u00b6 Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service> ImagePullPolicy \u00b6 By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#optional-debugging-lab-tips-and-tricks-for-debugging-applications-in-kubernetes","text":"Advanced debugging techniques to reach your pods.","title":"Optional Debugging Lab - Tips and Tricks for Debugging Applications in Kubernetes"},{"location":"generatedContent/kube101/LabD/#pod-logs","text":"You can look at the logs of any of the pods running under your deployments as follows kubectl logs <podname> Remember that if you have multiple containers running in your pod, you have to specify the specific container you want to see logs from. kubectl logs <pod-name> <container-name> This subcommand operates like tail . Including the -f flag will continue to stream the logs live once the current time is reached.","title":"Pod Logs"},{"location":"generatedContent/kube101/LabD/#kubectl-edit-and-vi","text":"By default, on many Linux and macOS systems, you will be dropped into the editor vi . export EDITOR = nano On Windows, a copy of notepad.exe will be opened with the contents of the file.","title":"kubectl edit and vi"},{"location":"generatedContent/kube101/LabD/#busybox-pod","text":"For debugging live, this command frequently helps me: kubectl create deployment bb --image busybox --restart = Never -it --rm In the busybox image is a basic shell that contains useful utilities. Utils I often use are nslookup and wget . nslookup is useful for testing DNS resolution in a pod. wget is useful for trying to do network requests.","title":"busybox pod"},{"location":"generatedContent/kube101/LabD/#service-endpoints","text":"Endpoint resource can be used to see all the service endpoints. kubectl get endpoints <service>","title":"Service Endpoints"},{"location":"generatedContent/kube101/LabD/#imagepullpolicy","text":"By default Kubernetes will only pull the image on first use. This can be confusing during development when you expect changes to show up. You should be aware of the three ImagePullPolicy s: IfNotPresent - the default, only request the image if not present. Always - always request the image. Never More details on image management may be found here .","title":"ImagePullPolicy"},{"location":"generatedContent/workshop-setup/","text":"Workshop Setup \u00b6 Setup instructions for the hands-on labs. Create IBM Cloud account Setup CLI environment - IBM Cloud Shell Connect to RedHat OpenShift Kubernetes Service (ROKS) Other links: Account onboarding via email invite Free Kubernetes Cluster","title":"Setup instructions"},{"location":"generatedContent/workshop-setup/#workshop-setup","text":"Setup instructions for the hands-on labs. Create IBM Cloud account Setup CLI environment - IBM Cloud Shell Connect to RedHat OpenShift Kubernetes Service (ROKS) Other links: Account onboarding via email invite Free Kubernetes Cluster","title":"Workshop Setup"},{"location":"generatedContent/workshop-setup/CALICO/","text":"Calico \u00b6 IBM Cloud Kubernetes Service (IKS) \u00b6 Calico is installed by default on IKS. To install the calicoctl client follow the instructions below. Connect to your Kubernetes cluster to start using the calicoctl . If you need help to setup your free IBM Cloud Kubernetes Service (IKS) cluster, go here , or to setup your RedHat OpenShift Kubernetes Service (ROKS), go here . calicoctl \u00b6 The following commands will install the calicoctl client in the browser-based terminal environment at CognitiveClass.ai . If you need help setting up the client terminal at CognitiveClass.ai, follow the instructions here . mkdir calico wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl -P ./calico chmod +x calico/calicoctl echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Unable to retrieve Cluster Version or Type: connection is unauthorized: clusterinformations.crd.projectcalico.org \"default\" is forbidden: User \"system:serviceaccount:sn-labs-remkohdev:remkohdev\" cannot get resource \"clusterinformations\" in API group \"crd.projectcalico.org\" at the cluster scope As kubectl Plugin \u00b6 To install the calicoctl as a plugin to the kubectl client, run the following commands, mkdir calico curl -o ./calico/kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x ./calico/kubectl-calico echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile kubectl calico -h","title":"Calico"},{"location":"generatedContent/workshop-setup/CALICO/#calico","text":"","title":"Calico"},{"location":"generatedContent/workshop-setup/CALICO/#ibm-cloud-kubernetes-service-iks","text":"Calico is installed by default on IKS. To install the calicoctl client follow the instructions below. Connect to your Kubernetes cluster to start using the calicoctl . If you need help to setup your free IBM Cloud Kubernetes Service (IKS) cluster, go here , or to setup your RedHat OpenShift Kubernetes Service (ROKS), go here .","title":"IBM Cloud Kubernetes Service (IKS)"},{"location":"generatedContent/workshop-setup/CALICO/#calicoctl","text":"The following commands will install the calicoctl client in the browser-based terminal environment at CognitiveClass.ai . If you need help setting up the client terminal at CognitiveClass.ai, follow the instructions here . mkdir calico wget https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl -P ./calico chmod +x calico/calicoctl echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile calicoctl version Client Version: v3.17.1 Git commit: 8871aca3 Unable to retrieve Cluster Version or Type: connection is unauthorized: clusterinformations.crd.projectcalico.org \"default\" is forbidden: User \"system:serviceaccount:sn-labs-remkohdev:remkohdev\" cannot get resource \"clusterinformations\" in API group \"crd.projectcalico.org\" at the cluster scope","title":"calicoctl"},{"location":"generatedContent/workshop-setup/CALICO/#as-kubectl-plugin","text":"To install the calicoctl as a plugin to the kubectl client, run the following commands, mkdir calico curl -o ./calico/kubectl-calico -L https://github.com/projectcalico/calicoctl/releases/download/v3.17.1/calicoctl chmod +x ./calico/kubectl-calico echo \"export PATH=$(pwd)/calico/:$PATH\" > $HOME/.bash_profile source $HOME/.bash_profile kubectl calico -h","title":"As kubectl Plugin"},{"location":"generatedContent/workshop-setup/CLIENT-CLUSTER/","text":"Claiming the Cluster \u00b6 You need an IBM Cloud account to access your cluster, Use the list provided by the instructor to find the name of your Kubernetes cluster. Open the Kubernetes cluster link to view all the availbale clusters: https://cloud.ibm.com/kubernetes/clusters?platformType=openshift Locate and click on the cluster that is assigned to you based on step 1.","title":"Claiming the Cluster"},{"location":"generatedContent/workshop-setup/CLIENT-CLUSTER/#claiming-the-cluster","text":"You need an IBM Cloud account to access your cluster, Use the list provided by the instructor to find the name of your Kubernetes cluster. Open the Kubernetes cluster link to view all the availbale clusters: https://cloud.ibm.com/kubernetes/clusters?platformType=openshift Locate and click on the cluster that is assigned to you based on step 1.","title":"Claiming the Cluster"},{"location":"generatedContent/workshop-setup/CLOUDSHELL/","text":"CLI Environment setup \u00b6 Most of the labs are run using CLI commands. The IBM Cloud Shell is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure. 1. Open IBM Cloud Shell \u00b6 Click on the IBM Cloud Shell icon at the top right section of the cloud account page. This opens the cloud shell in a new browser tab. 2. Setup Helm \u00b6 The Cloud Shell comes with both Helm 2 and Helm 3 versions. To make sure we use the Helm Version 3 variant, we will create an alias. Run the following commands to install Helm Version 3: alias helm = helm3 Check the helm version by running the following command: helm version --short The result is that you should have Helm Version 3 installed. $ helm version --short v3.2.1+gfe51cd1","title":"Setup CLI Environment"},{"location":"generatedContent/workshop-setup/CLOUDSHELL/#cli-environment-setup","text":"Most of the labs are run using CLI commands. The IBM Cloud Shell is preconfigured with the full IBM Cloud CLI and tons of plug-ins and tools that you can use to manage apps, resources, and infrastructure.","title":"CLI Environment setup"},{"location":"generatedContent/workshop-setup/CLOUDSHELL/#1-open-ibm-cloud-shell","text":"Click on the IBM Cloud Shell icon at the top right section of the cloud account page. This opens the cloud shell in a new browser tab.","title":"1. Open IBM Cloud Shell"},{"location":"generatedContent/workshop-setup/CLOUDSHELL/#2-setup-helm","text":"The Cloud Shell comes with both Helm 2 and Helm 3 versions. To make sure we use the Helm Version 3 variant, we will create an alias. Run the following commands to install Helm Version 3: alias helm = helm3 Check the helm version by running the following command: helm version --short The result is that you should have Helm Version 3 installed. $ helm version --short v3.2.1+gfe51cd1","title":"2. Setup Helm"},{"location":"generatedContent/workshop-setup/COGNITIVECLASS/","text":"CognitiveClass.ai \u00b6 Access CognitiveClass.ai \u00b6 If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai and login. Navigate to https://labs.cognitiveclass.ai/register , Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/COGNITIVECLASS/#cognitiveclassai","text":"","title":"CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/COGNITIVECLASS/#access-cognitiveclassai","text":"If you have already registered your account, you can access the lab environment at https://labs.cognitiveclass.ai and login. Navigate to https://labs.cognitiveclass.ai/register , Create a new account with your existing IBM Id. Alternative, you can choose to use a Social login (LinkedIn, Google, Github or Facebook), or for using your email account click the Cognitive Class button, Click Create an Account , Fill in your Email, Full Name, Public Username and password, click on the check boxes next to the Privacy Notice and Terms of Service to accept them. Then click on Create Account . You will then be taken to a page with a list of sandbox environments. Click on the option for Theia - Cloud IDE (With OpenShift) Wait a few minutes while your environment is created. You will be taken to a blank editor page once your environment is ready. What we really need is access to the terminal. Click on the Terminal tab near the top of the page and select New Terminal You can then click and drag the top of the terminal section upwards to make the terminal section bigger.","title":"Access CognitiveClass.ai"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/","text":"Create Free Kubernetes Cluster \u00b6 Prerequirements \u00b6 Free IBM Cloud account, to create a new IBM Cloud account go here . Free IBM Cloud Pay-As-You-Go account, to upgrade to a Pay-As-You-Go account go here IBM Cloud CLI with the Kubernetes Service plugin, see the IBM Cloud CLI Getting Started , or use a pre-installed client environment like the Labs environment at CognitiveClass, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here . Using UI \u00b6 Log in to IBM Cloud , Go to the Services Catalog , Filter the services by Kubernetes , Click the Kubernetes Service tile, Or go to the Create Kubernetes Service page directly, Configure the Kubernetes cluster as follows: For Pricing plan select Free . The page options will reload, leaving the default Kubernetes version as your only option, At the time of writing, the default Kubernetes version was set to 1.18.13 , Under Orchestration service , edit the Cluster name to a globally unique name, I recommend to follow a format like username-iks118-1n-cluster1 , where iks118 represents your Kubernetes version, 1n the number of worker nodes, and cluster1 represents your cluster number, in case you have more than 1 cluster. For Resource Group keep the Default resource group, unless you have created a new resource group and want to use your own resource group, Click Create to initiate the create cluster request, You will be forwarded to the Access details for the new cluster, Using CLI \u00b6 To create a free Kubernetes Service, you need to be logged in to a free IBM Cloud Pay-As-You-Go account. IBMID=<your ibm id email> USERNAME=<your short username> KS_CLUSTER_NAME=$USERNAME-iks118-1n-cluster1 KS_ZONE=dal10 KS_VERSION=1.18 KS_FLAVOR=u3c.2x4 KS_WORKERS=1 KS_PROVIDER=classic ibmcloud ks zone ls --provider $KS_PROVIDER ibmcloud ks flavors --zone $KS_ZONE --provider $KS_PROVIDER ibmcloud ks versions ibmcloud login -u $IBMID ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS The response should display similar output as, $ ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS Creating cluster... OK Cluster created with ID bvlntf2d0fe4l9hnres0 Retrieve details of the new cluster, ibmcloud ks cluster get --cluster $KS_CLUSTER_NAME --output json","title":"Create Free Kubernetes Cluster"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#create-free-kubernetes-cluster","text":"","title":"Create Free Kubernetes Cluster"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#prerequirements","text":"Free IBM Cloud account, to create a new IBM Cloud account go here . Free IBM Cloud Pay-As-You-Go account, to upgrade to a Pay-As-You-Go account go here IBM Cloud CLI with the Kubernetes Service plugin, see the IBM Cloud CLI Getting Started , or use a pre-installed client environment like the Labs environment at CognitiveClass, CognitiveLabs.ai account, to access a client terminal at CognitiveLabs.ai, go here .","title":"Prerequirements"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#using-ui","text":"Log in to IBM Cloud , Go to the Services Catalog , Filter the services by Kubernetes , Click the Kubernetes Service tile, Or go to the Create Kubernetes Service page directly, Configure the Kubernetes cluster as follows: For Pricing plan select Free . The page options will reload, leaving the default Kubernetes version as your only option, At the time of writing, the default Kubernetes version was set to 1.18.13 , Under Orchestration service , edit the Cluster name to a globally unique name, I recommend to follow a format like username-iks118-1n-cluster1 , where iks118 represents your Kubernetes version, 1n the number of worker nodes, and cluster1 represents your cluster number, in case you have more than 1 cluster. For Resource Group keep the Default resource group, unless you have created a new resource group and want to use your own resource group, Click Create to initiate the create cluster request, You will be forwarded to the Access details for the new cluster,","title":"Using UI"},{"location":"generatedContent/workshop-setup/FREEIKSCLUSTER/#using-cli","text":"To create a free Kubernetes Service, you need to be logged in to a free IBM Cloud Pay-As-You-Go account. IBMID=<your ibm id email> USERNAME=<your short username> KS_CLUSTER_NAME=$USERNAME-iks118-1n-cluster1 KS_ZONE=dal10 KS_VERSION=1.18 KS_FLAVOR=u3c.2x4 KS_WORKERS=1 KS_PROVIDER=classic ibmcloud ks zone ls --provider $KS_PROVIDER ibmcloud ks flavors --zone $KS_ZONE --provider $KS_PROVIDER ibmcloud ks versions ibmcloud login -u $IBMID ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS The response should display similar output as, $ ibmcloud ks cluster create $KS_PROVIDER --name $KS_CLUSTER_NAME --zone $KS_ZONE --version $KS_VERSION --flavor $KS_FLAVOR --workers $KS_WORKERS Creating cluster... OK Cluster created with ID bvlntf2d0fe4l9hnres0 Retrieve details of the new cluster, ibmcloud ks cluster get --cluster $KS_CLUSTER_NAME --output json","title":"Using CLI"},{"location":"generatedContent/workshop-setup/GRANTCLUSTER/","text":"Grant Cluster \u00b6 IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS) \u00b6 The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"Grant Cluster"},{"location":"generatedContent/workshop-setup/GRANTCLUSTER/#grant-cluster","text":"","title":"Grant Cluster"},{"location":"generatedContent/workshop-setup/GRANTCLUSTER/#ibm-kubernetes-service-iks-and-redhat-openshift-kubernetes-service-roks","text":"The grant cluster method to get access to a Kubernetes cluster will assign access permissions to a cluster or namespace in a cluster that was created prior to the request. Creating a cluster and provisioning the VMs and other resources and deploying the tools may take up to 15 minutes and longer if queued. Permissioning access to an existing cluster in contrast happens in 1 or 2 minutes depending on the number of concurrent requests. You need an IBM Cloud account to access your cluster, If you do not have an IBM Cloud account yet, register at https://cloud.ibm.com/registration , Or find instructions to create a new IBM Cloud account here , To grant a cluster, You need to be given a URL to submit your grant cluster request, Open the URL to grant a cluster, e.g. https://<workshop>.mybluemix.net , The grant cluster URL should open the following page, Log in to this IBM Cloud account using the lab key given to you by the instructor and your IBM Id to access your IBM Cloud account, Instructions will ask to Log in to this IBM Cloud account When you click the link to log in to the IBM Cloud account, the IBM Cloud overview page will load with an overview of all resources on the account. In the top right, you will see an active account listed. The active account should be the account on which the cluster is created, which is not your personal account. Click the account dropdown if you need to change the active account. Navigate to Clusters, And select the cluster assigned to you... Details for your cluster will load, Go to the Access menu item in the left navigation column, Follow the instructions to access your cluster from the client, Optionally, you can use the IBM Cloud Shell at https://shell.cloud.ibm.com/ to check access. The cloud shell is attached to your IBM Id. It might take a few moments to create the instance and a new session,","title":"IBM Kubernetes Service (IKS) and RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/HELM/","text":"Helm v3 \u00b6 Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce","title":"Helm v3"},{"location":"generatedContent/workshop-setup/HELM/#helm-v3","text":"Refer to the Helm install docs for more details. To install Helm v3, run the following commands, In the Cloud Shell , download and unzip Helm v3.2. cd $HOME wget https://get.helm.sh/helm-v3.2.0-linux-amd64.tar.gz tar -zxvf helm-v3.2.0-linux-amd64.tar.gz Make Helm v3 CLI available in your PATH environment variable. echo 'export PATH=$HOME/linux-amd64:$PATH' > $HOME/.bash_profile source $HOME/.bash_profile Verify Helm v3 installation. helm version --short outputs, $ helm version --short v3.2.0+ge11b7ce","title":"Helm v3"},{"location":"generatedContent/workshop-setup/JENKINS/","text":"Jenkins \u00b6 Pre-requirements \u00b6 OpenShift 4.x cluster Setup \u00b6 From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator provided by Red Hat , labeled community , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Install Operator window, in the Update Channel section, select alpha under Update Channel , choose A specific namespace in the cluster and in the Installed Namespace section, select the project jenkins from the dropdown, select Automatic under Approval Strategy , Click Install , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins window, select Form View or YAML View for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, If your page loads with a Application is not available warning, your Jenkins instance is still being deployed and you need to wait a little longer, keep trying until the Jenkins page loads, You can see the progress of the Jenkins startup, by browsing to the Pods of the Deployment of the Jenkins instance that is being created, Click Log in with OpenShift , Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Jenkins"},{"location":"generatedContent/workshop-setup/JENKINS/#jenkins","text":"","title":"Jenkins"},{"location":"generatedContent/workshop-setup/JENKINS/#pre-requirements","text":"OpenShift 4.x cluster","title":"Pre-requirements"},{"location":"generatedContent/workshop-setup/JENKINS/#setup","text":"From the IBM Cloud cluster dashboard, click the OpenShift web console button, First we need to create a new project named jenkins to deploy the Jenkins service to, From the terminal, oc new-project jenkins outputs, $ oc new-project jenkins Now using project \"jenkins\" on server \"https://c107-e.us-south.containers.cloud.ibm.com:31608\". Or in the Openshift web console, go to Home > Projects , Click Create Project For Name enter jenkins , for Display Name enter jenkins , and for Description enter jenkins , Click Create , Go to Operators > OperatorHub , For Filter by keyword enter Jenkins , Select the Jenkins Operator provided by Red Hat , labeled community , Click Continue to Show Community Operator , Review the operator information, and click Install , In the Install Operator window, in the Update Channel section, select alpha under Update Channel , choose A specific namespace in the cluster and in the Installed Namespace section, select the project jenkins from the dropdown, select Automatic under Approval Strategy , Click Install , The Installed Operators page will load, wait until the Jenkins Operator has a Status of Succeeded , Click the installed operator linked Name of Jenkins Operator , In the Provided APIs section, click the Create Instance link in the Jenkins panel, In the Create Jenkins window, select Form View or YAML View for the new Jenkins instance, change the metadata.name to my-jenkins , accept all other specifications, Click Create , Go to Networking > Routes , and look for a new Route jenkins-my-jenkins , Click the link for jenkins-my-jenkins route in the Location column, A route to your Jenkins instance opens in a new browser window or tab, If your page loads with a Application is not available warning, your Jenkins instance is still being deployed and you need to wait a little longer, keep trying until the Jenkins page loads, You can see the progress of the Jenkins startup, by browsing to the Pods of the Deployment of the Jenkins instance that is being created, Click Log in with OpenShift , Click Allow selected permissions , Welcome to Jenkins ! Configure Jenkins Go to Jenkins > Manage Jenkins > Global Tool Configuration, Go to the Maven section, Click Maven Installations , If no Maven installer is configured, click Add Maven , Configure the Name to be maven , check the option Install automatically and select version 3.6.3 , Click Save,","title":"Setup"},{"location":"generatedContent/workshop-setup/MKDOCS/","text":"MkDocs \u00b6 Installation \u00b6 Go to the Installation instructions for MkDocs, alias python=python3 python --version alias pip=pip3 pip --version pip install --upgrade pip pip install mkdocs mkdocs --version To install Material for MkDocs go to the Getting Started instructions, pip install mkdocs-material In the repository root directory, add a new ~/README.md file, this is the landing page for the Github repository different from the MkDocs landing page. MkDocs assumes its root to be the docs folder. vi README.md Migration \u00b6 Get repository, ORG=<github_org> REPO=<repository> git clone https://github.com/$ORG/$REPO.git cd $REPO When converting an existing repository from Gitbook, rename workshop to docs , mv workshop docs For a repository without Gitbook support, create a new directory docs , mkdir docs When converting from Gitbook, edit .gitbook.yaml and change workshop to docs references, sed -i \"\" 's/workshop/docs/' .gitbook.yaml Skip this step for repositories with a docs/README.md file present! If no file docs/README.md exists, add one. The docs/README.md is the landing page for the MkDocs documentation, cat > docs/README.md <<EOF # $REPO # # About EOF Create a new file .github/workflows/ci.yml to configure a Github action, mkdir -p .github/workflows wget -O .github/workflows/ci.yml https://raw.githubusercontent.com/IBM/workshop-setup/master/.github/workflows/ci.yml In the root folder of your project, create a new file ~/mkdocs.yml and add the following content to the new file, cat > mkdocs.yml <<EOF # Project information site_name: <SITE_NAME> site_url: https://ibm.github.io/<REPO_NAME> site_author: IBM Developer # Repository repo_name: <REPO_NAME> repo_url: https://github.com/ibm/<REPO_NAME> edit_uri: edit/master/docs # Navigation nav: - Welcome: - About the workshop: README.md - Workshop: - Lab 1: lab1.md - Lab 2: lab2.md - References: - Additional resources: references/RESOURCES.md - Contributors: references/CONTRIBUTORS.md # # DO NOT CHANGE BELOW THIS LINE # Copyright copyright: Copyright &copy; 2020 IBM Developer # Theme theme: name: material font: text: IBM Plex Sans code: IBM Plex Mono icon: logo: material/library features: - navigation.tabs # - navigation.instant palette: scheme: default primary: blue accent: blue # Plugins plugins: - search # Customization extra: social: - icon: fontawesome/brands/github link: https://github.com/ibm - icon: fontawesome/brands/twitter link: https://twitter.com/ibmdeveloper - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/company/ibm/ - icon: fontawesome/brands/youtube link: https://www.youtube.com/user/developerworks - icon: fontawesome/brands/dev link: https://dev.to/ibmdeveloper # Extensions markdown_extensions: - abbr - admonition - attr_list - def_list - footnotes - meta - toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences: custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde EOF Edit the above file mkdocs.yml , change: <SITE_NAME> <REPO_NAME> when converting from Gitbook: copy the navigation links from workshop/SUMMARY.md to the Navigation section and copy-edit the links for the navigation, sed -i \"\" \"s/<SITE_NAME>/$REPO/\" mkdocs.yml sed -i \"\" \"s/<REPO_NAME>/$REPO/\" mkdocs.yml The above navigation example includes two example items, you need to add the files docs/lab1.md and docs/lab2.md to enable the examples, echo '# Lab One' > docs/lab1.md echo '# Lab Two' > docs/lab2.md But for existing content, you need to overwrite the navigation. If your repository has an existing Gitbook, you can copy the navigation in the docs/SUMMARY.md file, to the # Navigation nav: section in mkdocs.yml , Add a file .gitignore with the following exceptions, echo \"\\n# MkDocs\\nsite/\\n\" >> .gitignore Add a new file ~/markdownlint.json with the following rule, cat > markdownlint.json <<EOF { \"line-length\": false } EOF Add a new file .markdownlintignore , when converting from Gitbook, add the following exceptions, cat > .markdownlintignore <<EOF workshop/SUMMARY.md workshop/README.md EOF When migrating from Gitbook , if a .travis.yml file already exists, replace reference to workshop by docs , sed -i \"\" 's/workshop/docs/' .travis.yml Add a new file .travis.yml , cat > .travis.yml <<EOF --- language: node_js node_js: 10 before_script: - npm install markdownlint-cli script: - markdownlint -c .markdownlint.json docs EOF If you don't have a LICENSE file yet, download the LICENSE file, wget -O LICENSE https://raw.githubusercontent.com/IBM/workshop-setup/master/LICENSE Test the docs locally with the command mkdocs serve , Review the WARNING output, fix where necessary, Create a new repository in Github, Initialize the local repository, add all files, commit, add the remote, and push changes, echo \"# test1\" >> README.md git init git add . git commit -m \"first commit\" git branch -M main git remote add origin https://github.com/remkohdev/test1.git git push -u origin main Github pages use a branch called gh-pages , this branch is automatically created if it does not exist by the Github action that is defined in the .github/workflows/ci.yml , Github also needs to resolve the domain name and URI to the repository, and point it to the correct branch of the Github page for the repo. To configure this, in your repo, go to Settings, scroll down to GitHub Pages section and from the Source dropdown, select the branch gh-pages , accept the default /root and click Save . Your MkDocs branch should now be live. Whenever you push changes to your main branch, the Github action to deploy MkDocs will be triggered again, Add an update to your README.md, vi README.md Push changes, git add . git commit -m \"second commit\" git push Check your actions at https://github.com/<ORG>/<REPO>/actions . Export to PDF \u00b6 There are several plugins to export mkdocs to PDF, mkdocs-pdf-export , MkPdf , Install the plugin, Enable the plugin by adding the plugin line to the plugins list in mkdocs.yml , to use the pdf-export plugin, add, plugins: - pdf-export: verbose: true media_type: print enabled_if_env: ENABLE_PDF_EXPORT combined: true Run mkdocs build , which will create a pdf directory with the combined PDF, e.g. to run pdf-export , ENABLE_PDF_EXPORT=1 mkdocs build","title":"MkDocs"},{"location":"generatedContent/workshop-setup/MKDOCS/#mkdocs","text":"","title":"MkDocs"},{"location":"generatedContent/workshop-setup/MKDOCS/#installation","text":"Go to the Installation instructions for MkDocs, alias python=python3 python --version alias pip=pip3 pip --version pip install --upgrade pip pip install mkdocs mkdocs --version To install Material for MkDocs go to the Getting Started instructions, pip install mkdocs-material In the repository root directory, add a new ~/README.md file, this is the landing page for the Github repository different from the MkDocs landing page. MkDocs assumes its root to be the docs folder. vi README.md","title":"Installation"},{"location":"generatedContent/workshop-setup/MKDOCS/#migration","text":"Get repository, ORG=<github_org> REPO=<repository> git clone https://github.com/$ORG/$REPO.git cd $REPO When converting an existing repository from Gitbook, rename workshop to docs , mv workshop docs For a repository without Gitbook support, create a new directory docs , mkdir docs When converting from Gitbook, edit .gitbook.yaml and change workshop to docs references, sed -i \"\" 's/workshop/docs/' .gitbook.yaml Skip this step for repositories with a docs/README.md file present! If no file docs/README.md exists, add one. The docs/README.md is the landing page for the MkDocs documentation, cat > docs/README.md <<EOF # $REPO # # About EOF Create a new file .github/workflows/ci.yml to configure a Github action, mkdir -p .github/workflows wget -O .github/workflows/ci.yml https://raw.githubusercontent.com/IBM/workshop-setup/master/.github/workflows/ci.yml In the root folder of your project, create a new file ~/mkdocs.yml and add the following content to the new file, cat > mkdocs.yml <<EOF # Project information site_name: <SITE_NAME> site_url: https://ibm.github.io/<REPO_NAME> site_author: IBM Developer # Repository repo_name: <REPO_NAME> repo_url: https://github.com/ibm/<REPO_NAME> edit_uri: edit/master/docs # Navigation nav: - Welcome: - About the workshop: README.md - Workshop: - Lab 1: lab1.md - Lab 2: lab2.md - References: - Additional resources: references/RESOURCES.md - Contributors: references/CONTRIBUTORS.md # # DO NOT CHANGE BELOW THIS LINE # Copyright copyright: Copyright &copy; 2020 IBM Developer # Theme theme: name: material font: text: IBM Plex Sans code: IBM Plex Mono icon: logo: material/library features: - navigation.tabs # - navigation.instant palette: scheme: default primary: blue accent: blue # Plugins plugins: - search # Customization extra: social: - icon: fontawesome/brands/github link: https://github.com/ibm - icon: fontawesome/brands/twitter link: https://twitter.com/ibmdeveloper - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/company/ibm/ - icon: fontawesome/brands/youtube link: https://www.youtube.com/user/developerworks - icon: fontawesome/brands/dev link: https://dev.to/ibmdeveloper # Extensions markdown_extensions: - abbr - admonition - attr_list - def_list - footnotes - meta - toc: permalink: true - pymdownx.arithmatex: generic: true - pymdownx.betterem: smart_enable: all - pymdownx.caret - pymdownx.critic - pymdownx.details - pymdownx.emoji: emoji_index: !!python/name:materialx.emoji.twemoji emoji_generator: !!python/name:materialx.emoji.to_svg - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.keys - pymdownx.mark - pymdownx.smartsymbols - pymdownx.snippets: check_paths: true - pymdownx.superfences: custom_fences: - name: mermaid class: mermaid format: !!python/name:pymdownx.superfences.fence_code_format - pymdownx.tabbed - pymdownx.tasklist: custom_checkbox: true - pymdownx.tilde EOF Edit the above file mkdocs.yml , change: <SITE_NAME> <REPO_NAME> when converting from Gitbook: copy the navigation links from workshop/SUMMARY.md to the Navigation section and copy-edit the links for the navigation, sed -i \"\" \"s/<SITE_NAME>/$REPO/\" mkdocs.yml sed -i \"\" \"s/<REPO_NAME>/$REPO/\" mkdocs.yml The above navigation example includes two example items, you need to add the files docs/lab1.md and docs/lab2.md to enable the examples, echo '# Lab One' > docs/lab1.md echo '# Lab Two' > docs/lab2.md But for existing content, you need to overwrite the navigation. If your repository has an existing Gitbook, you can copy the navigation in the docs/SUMMARY.md file, to the # Navigation nav: section in mkdocs.yml , Add a file .gitignore with the following exceptions, echo \"\\n# MkDocs\\nsite/\\n\" >> .gitignore Add a new file ~/markdownlint.json with the following rule, cat > markdownlint.json <<EOF { \"line-length\": false } EOF Add a new file .markdownlintignore , when converting from Gitbook, add the following exceptions, cat > .markdownlintignore <<EOF workshop/SUMMARY.md workshop/README.md EOF When migrating from Gitbook , if a .travis.yml file already exists, replace reference to workshop by docs , sed -i \"\" 's/workshop/docs/' .travis.yml Add a new file .travis.yml , cat > .travis.yml <<EOF --- language: node_js node_js: 10 before_script: - npm install markdownlint-cli script: - markdownlint -c .markdownlint.json docs EOF If you don't have a LICENSE file yet, download the LICENSE file, wget -O LICENSE https://raw.githubusercontent.com/IBM/workshop-setup/master/LICENSE Test the docs locally with the command mkdocs serve , Review the WARNING output, fix where necessary, Create a new repository in Github, Initialize the local repository, add all files, commit, add the remote, and push changes, echo \"# test1\" >> README.md git init git add . git commit -m \"first commit\" git branch -M main git remote add origin https://github.com/remkohdev/test1.git git push -u origin main Github pages use a branch called gh-pages , this branch is automatically created if it does not exist by the Github action that is defined in the .github/workflows/ci.yml , Github also needs to resolve the domain name and URI to the repository, and point it to the correct branch of the Github page for the repo. To configure this, in your repo, go to Settings, scroll down to GitHub Pages section and from the Source dropdown, select the branch gh-pages , accept the default /root and click Save . Your MkDocs branch should now be live. Whenever you push changes to your main branch, the Github action to deploy MkDocs will be triggered again, Add an update to your README.md, vi README.md Push changes, git add . git commit -m \"second commit\" git push Check your actions at https://github.com/<ORG>/<REPO>/actions .","title":"Migration"},{"location":"generatedContent/workshop-setup/MKDOCS/#export-to-pdf","text":"There are several plugins to export mkdocs to PDF, mkdocs-pdf-export , MkPdf , Install the plugin, Enable the plugin by adding the plugin line to the plugins list in mkdocs.yml , to use the pdf-export plugin, add, plugins: - pdf-export: verbose: true media_type: print enabled_if_env: ENABLE_PDF_EXPORT combined: true Run mkdocs build , which will create a pdf directory with the combined PDF, e.g. to run pdf-export , ENABLE_PDF_EXPORT=1 mkdocs build","title":"Export to PDF"},{"location":"generatedContent/workshop-setup/NEWACCOUNT-invite/","text":"Create IBM Cloud ID and Account onboarding \u00b6 Follow the steps below to complete the onboarding process: Open the email invite and click on the Join now link. In the Join account window, enter your name and the password you would like to use. Click the Join Account button. At the Account activation confirmation page, click the Log in button. Enter the account landing page with the Dashboard on dislay. At the top, towards right, the account information is shown. You have successfully onboarded the IBM Cloud account.","title":"Create IBM Cloud ID and Account onboarding"},{"location":"generatedContent/workshop-setup/NEWACCOUNT-invite/#create-ibm-cloud-id-and-account-onboarding","text":"Follow the steps below to complete the onboarding process: Open the email invite and click on the Join now link. In the Join account window, enter your name and the password you would like to use. Click the Join Account button. At the Account activation confirmation page, click the Log in button. Enter the account landing page with the Dashboard on dislay. At the top, towards right, the account information is shown. You have successfully onboarded the IBM Cloud account.","title":"Create IBM Cloud ID and Account onboarding"},{"location":"generatedContent/workshop-setup/NEWACCOUNT/","text":"Create IBM Cloud ID / Account \u00b6 To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"Create IBM Cloud Account"},{"location":"generatedContent/workshop-setup/NEWACCOUNT/#create-ibm-cloud-id-account","text":"To create a new account, follow the steps below, Open a web browser and go to the IBM Cloud Sign up page In the Create an account window, enter your company email id and the password you would like to use. Click the Next button. The Verify email section will inform you that a verification code was sent to your email. Switch to your email provider to retrieve the verification code. Then enter the verification code in the Verify email section, and click the Next button. Enter your first name, last name and country in the Personal information section and click the Next button. Click the Create account button. After your account is created, review the IBM Privacy Statement . Then scroll down and click the Proceed button to acknowledge the privacy statement. You are now ready to login to the IBM Cloud. Open a web browser to the IBM Cloud console . If prompted, enter your IBM Id (the email ID you used to create the account above) followed by your password to login. The IBM Cloud dashboard page should load. You have successfully registered a new IBM Cloud account.","title":"Create IBM Cloud ID / Account"},{"location":"generatedContent/workshop-setup/OPENLABS/","text":"Access OpenShift Cluster at OpenLabs \u00b6 These instructions will walk you through how you can get access to a 2 node OpenShift cluster on IBM Cloud through IBM OpenLabs. This cluster will only be available for 2 hours and then will be deleted. Go to IBM OpenLabs: https://developer.ibm.com/openlabs/openshift Find the lab in the Hands on Labs section called Lab 6: Bring Your Own App You will be asked to sign into IBM Cloud if you aren't already or you can register for a new account. Once you sign in the lab will automatically continue provisioning. You may see messages like Allocating VM and so on. When that is done you should be taken to a new page that has documentation on the left half of the screen along with a terminal environment on the right. If not, click on the Lab 6: Bring Your Own App button again. To access your new OpenShift Cluster, click on the tab on the left side of the page labeled Quick Links and Common Commands From this page you can access the OpenShift Web Console and find instructions on how to log in to the cluster in the terminal with the oc cli tool.","title":"Access OpenShift Cluster at OpenLabs"},{"location":"generatedContent/workshop-setup/OPENLABS/#access-openshift-cluster-at-openlabs","text":"These instructions will walk you through how you can get access to a 2 node OpenShift cluster on IBM Cloud through IBM OpenLabs. This cluster will only be available for 2 hours and then will be deleted. Go to IBM OpenLabs: https://developer.ibm.com/openlabs/openshift Find the lab in the Hands on Labs section called Lab 6: Bring Your Own App You will be asked to sign into IBM Cloud if you aren't already or you can register for a new account. Once you sign in the lab will automatically continue provisioning. You may see messages like Allocating VM and so on. When that is done you should be taken to a new page that has documentation on the left half of the screen along with a terminal environment on the right. If not, click on the Lab 6: Bring Your Own App button again. To access your new OpenShift Cluster, click on the tab on the left side of the page labeled Quick Links and Common Commands From this page you can access the OpenShift Web Console and find instructions on how to log in to the cluster in the terminal with the oc cli tool.","title":"Access OpenShift Cluster at OpenLabs"},{"location":"generatedContent/workshop-setup/PAYASYOUGO/","text":"Upgrade to Pay-As-You-Go Account \u00b6 Login to your IBM Cloud account , Go to account settings , Click the Add credit card button, to create the Pay-As-You-Go account, which will unlock the full catalog, Select the Account type , choose between Company and Personal . For the setup, I will choose Personal . Click Next , Enter your Billing Information , click Next , Enter your Payment information , click Next , Acknowledge that your card will not be charged, but a $1.00 hold will be placed while authorizing the card, Click Upgrade Account , You will receive $200 credit, in case you want to use paid services, Click Done , Under Account settings you should now see Account Type as Pay-As-You-Go . Go to the IBM Cloud Catalog and filter Free and Lite services . Free and Lite Services \u00b6 At the time of writing this setup 115 free and lite services were listed, among other: Analytics Engine, API Connect, API Gateway, App Connect, App ID, Auto Scale for VPC, Certificate Manager, Cloudant, Code Engine, Container Registry, Continuous Delivery, DB2, Direct Link Connect, Discovery, Event Streams with Kafka, Hyper Protect, LogDNA, Sysdig, IBM Cognos, Internet of Things Platform, Internet Services with CloudFlare, Key Protect, Knowledge Studio, Kubernetes Service, Language Translator, Load Balancer for VPC, Machine Learning, MQ, Natural Language Classifier, Natural Language Understanding, Object Storage, PagerDuty, Schematics with Terraform, Secrets Manager, Secure Gateway, Speech to Text, SQL Query, Object Storage with ANSI SQL, Streaming Analytics, Text to Speech, Tone Analyzer, Toolchain, Twilio, Visual Recognition, Voice Agent with Watson, Watson Assistant, Watson Knowledge Catalog, Watson OpenScale, Watson Studio,","title":"Upgrade to Pay-As-You-Go Account"},{"location":"generatedContent/workshop-setup/PAYASYOUGO/#upgrade-to-pay-as-you-go-account","text":"Login to your IBM Cloud account , Go to account settings , Click the Add credit card button, to create the Pay-As-You-Go account, which will unlock the full catalog, Select the Account type , choose between Company and Personal . For the setup, I will choose Personal . Click Next , Enter your Billing Information , click Next , Enter your Payment information , click Next , Acknowledge that your card will not be charged, but a $1.00 hold will be placed while authorizing the card, Click Upgrade Account , You will receive $200 credit, in case you want to use paid services, Click Done , Under Account settings you should now see Account Type as Pay-As-You-Go . Go to the IBM Cloud Catalog and filter Free and Lite services .","title":"Upgrade to Pay-As-You-Go Account"},{"location":"generatedContent/workshop-setup/PAYASYOUGO/#free-and-lite-services","text":"At the time of writing this setup 115 free and lite services were listed, among other: Analytics Engine, API Connect, API Gateway, App Connect, App ID, Auto Scale for VPC, Certificate Manager, Cloudant, Code Engine, Container Registry, Continuous Delivery, DB2, Direct Link Connect, Discovery, Event Streams with Kafka, Hyper Protect, LogDNA, Sysdig, IBM Cognos, Internet of Things Platform, Internet Services with CloudFlare, Key Protect, Knowledge Studio, Kubernetes Service, Language Translator, Load Balancer for VPC, Machine Learning, MQ, Natural Language Classifier, Natural Language Understanding, Object Storage, PagerDuty, Schematics with Terraform, Secrets Manager, Secure Gateway, Speech to Text, SQL Query, Object Storage with ANSI SQL, Streaming Analytics, Text to Speech, Tone Analyzer, Toolchain, Twilio, Visual Recognition, Voice Agent with Watson, Watson Assistant, Watson Knowledge Catalog, Watson OpenScale, Watson Studio,","title":"Free and Lite Services"},{"location":"generatedContent/workshop-setup/ROKS/","text":"Connect to RedHat OpenShift Kubernetes Service (ROKS) \u00b6 Claiming the Cluster \u00b6 Use the list provided by the instructor to find the name of your Kubernetes cluster. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#. Open the Kubernetes cluster link to view all the availbale clusters: https://cloud.ibm.com/kubernetes/clusters?platformType=openshift Locate and click on the cluster that is assigned to you based on step 1. Login to OpenShift \u00b6 Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Output of the login command: $ oc login --token = nAH06HFM04vlTNNHor3QJ_CMySawfSsV4CNUlwoMUKM --server = https://c115-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c115-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#rojanjose@gmail.com\" using the token provided. You have access to 62 projects, the list has been suppressed. You can list all projects with ' projects' Using project \"default\". Welcome! See 'oc help' to get started. Verify you connect to the right cluster. oc get all oc get nodes -o wide Output: $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 25h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 25h service/openshift-apiserver ClusterIP 172.21.204.216 <none> 443/TCP 25h $ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.171.109.110 Ready master,worker 25h v1.18.3+cdb0358 10.171.109.110 169.46.60.55 Red Hat 3.10.0-1160.24.1.el7.x86_64 cri-o://1.18.4-11.rhaos4.5.gitfa57051.el7 10.171.109.113 Ready master,worker 25h v1.18.3+cdb0358 10.171.109.113 169.46.60.57 Red Hat 3.10.0-1160.24.1.el7.x86_64 cri-o://1.18.4-11.rhaos4.5.gitfa57051.el7 Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME=<your_cluster_name> Output: $ export CLUSTER_NAME = openshift-cluster-user-0","title":"Connect to cluster"},{"location":"generatedContent/workshop-setup/ROKS/#connect-to-redhat-openshift-kubernetes-service-roks","text":"","title":"Connect to RedHat OpenShift Kubernetes Service (ROKS)"},{"location":"generatedContent/workshop-setup/ROKS/#claiming-the-cluster","text":"Use the list provided by the instructor to find the name of your Kubernetes cluster. Make sure that you are in the correct account#. Note: you may not have access to your OpenShift cluster if you are not in the right account#. Open the Kubernetes cluster link to view all the availbale clusters: https://cloud.ibm.com/kubernetes/clusters?platformType=openshift Locate and click on the cluster that is assigned to you based on step 1.","title":"Claiming the Cluster"},{"location":"generatedContent/workshop-setup/ROKS/#login-to-openshift","text":"Click OpenShift web console button on the top. Click on your username in the upper right and select Copy Login Command option. Click the Display Token link. Copy the contents of the field Log in with this token to the clipboard. It provides a login command with a valid token for your username. Go to the your shell terminal. Paste the oc login command in the IBM Cloud Shell terminal and run it. Output of the login command: $ oc login --token = nAH06HFM04vlTNNHor3QJ_CMySawfSsV4CNUlwoMUKM --server = https://c115-e.us-south.containers.cloud.ibm.com:30979 Logged into \"https://c115-e.us-south.containers.cloud.ibm.com:30979\" as \"IAM#rojanjose@gmail.com\" using the token provided. You have access to 62 projects, the list has been suppressed. You can list all projects with ' projects' Using project \"default\". Welcome! See 'oc help' to get started. Verify you connect to the right cluster. oc get all oc get nodes -o wide Output: $ oc get all NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 172.21.0.1 <none> 443/TCP 25h service/openshift ExternalName <none> kubernetes.default.svc.cluster.local <none> 25h service/openshift-apiserver ClusterIP 172.21.204.216 <none> 443/TCP 25h $ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.171.109.110 Ready master,worker 25h v1.18.3+cdb0358 10.171.109.110 169.46.60.55 Red Hat 3.10.0-1160.24.1.el7.x86_64 cri-o://1.18.4-11.rhaos4.5.gitfa57051.el7 10.171.109.113 Ready master,worker 25h v1.18.3+cdb0358 10.171.109.113 169.46.60.57 Red Hat 3.10.0-1160.24.1.el7.x86_64 cri-o://1.18.4-11.rhaos4.5.gitfa57051.el7 Optionally, for convenience, set an environment variable for your cluster name. export CLUSTER_NAME=<your_cluster_name> Output: $ export CLUSTER_NAME = openshift-cluster-user-0","title":"Login to OpenShift"},{"location":"generatedContent/workshop-setup/S2I/","text":"Source-to-Image (S2I) \u00b6 To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file sudo tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"Source-to-Image (S2I)"},{"location":"generatedContent/workshop-setup/S2I/#source-to-image-s2i","text":"To install s2i CLI tool, Download tar file. curl -s https://api.github.com/repos/openshift/source-to-image/releases/latest \\ | grep browser_download_url \\ | grep linux-amd64 \\ | cut -d '\"' -f 4 \\ | wget -qi - Unzip tar file sudo tar xvf source-to-image*.gz Make s2i CLI accessiblee. sudo mv s2i /usr/local/bin verify s2i version With that done, you can start the lab.","title":"Source-to-Image (S2I)"},{"location":"generatedContent/workshop-setup/references/CONTRIBUTORS/","text":"Contributors \u00b6 Remko de Knikker Github: remkohdev ] Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev Steve Martinelli John Zaccone","title":"Contributors"},{"location":"generatedContent/workshop-setup/references/CONTRIBUTORS/#contributors","text":"Remko de Knikker Github: remkohdev ] Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev Steve Martinelli John Zaccone","title":"Contributors"},{"location":"generatedContent/workshop-setup/references/RESOURCES/","text":"Additional resources \u00b6 IBM Demos \u00b6 Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage Redbooks \u00b6 IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices Videos \u00b6 Video: Postal codes and part numbers (DataStage) Video: Calculate employee compensation (read from CSV) (DataStage and Gov. Catalog) Video: Banks have merged (DataStage and Gov. Catalog) Video: Groceries with Kafka (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Additional resources"},{"location":"generatedContent/workshop-setup/references/RESOURCES/#additional-resources","text":"","title":"Additional resources"},{"location":"generatedContent/workshop-setup/references/RESOURCES/#ibm-demos","text":"Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage","title":"IBM Demos"},{"location":"generatedContent/workshop-setup/references/RESOURCES/#redbooks","text":"IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices","title":"Redbooks"},{"location":"generatedContent/workshop-setup/references/RESOURCES/#videos","text":"Video: Postal codes and part numbers (DataStage) Video: Calculate employee compensation (read from CSV) (DataStage and Gov. Catalog) Video: Banks have merged (DataStage and Gov. Catalog) Video: Groceries with Kafka (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Videos"},{"location":"pre-work/","text":"Pre-work \u00b6 This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo 1. Sign up for IBM Cloud \u00b6 Ensure you have an IBM Cloud ID 2. Download or clone the repo \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"Pre-work"},{"location":"pre-work/#pre-work","text":"This section is broken up into the following steps: Sign up for IBM Cloud Download or clone the repo","title":"Pre-work"},{"location":"pre-work/#1-sign-up-for-ibm-cloud","text":"Ensure you have an IBM Cloud ID","title":"1. Sign up for IBM Cloud"},{"location":"pre-work/#2-download-or-clone-the-repo","text":"Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Alternately, run the following command: git clone https://github.com/IBM/workshop-template cd workshop-template","title":"2. Download or clone the repo"},{"location":"resources/ADMIN/","text":"Admin Guide \u00b6 This section is comprised of the following steps: Instructor Step 1. Instructor Step \u00b6 Things specific to instructors can go here.","title":"Admin Guide"},{"location":"resources/ADMIN/#admin-guide","text":"This section is comprised of the following steps: Instructor Step","title":"Admin Guide"},{"location":"resources/ADMIN/#1-instructor-step","text":"Things specific to instructors can go here.","title":"1. Instructor Step"},{"location":"resources/CONTRIBUTORS/","text":"Contributors \u00b6 Remko de Knikker \u00b6 Github: remkohdev Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev Steve Martinelli \u00b6 Github: stevemar Twitter: @stevebot LinkedIn: stevemar","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#contributors","text":"","title":"Contributors"},{"location":"resources/CONTRIBUTORS/#remko-de-knikker","text":"Github: remkohdev Twitter: @remkohdev LinkedIn: remkohdev Medium: @remkohdev","title":"Remko de Knikker"},{"location":"resources/CONTRIBUTORS/#steve-martinelli","text":"Github: stevemar Twitter: @stevebot LinkedIn: stevemar","title":"Steve Martinelli"},{"location":"resources/MKDOCS/","text":"mkdocs examples \u00b6 This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation . Code \u00b6 print ( \"hello world!\" ) Code with line numbers \u00b6 1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with highlights \u00b6 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Code with tabs \u00b6 Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } More tabs \u00b6 Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo . Checklists \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst Add a button \u00b6 Launch the lab Visit IBM Developer Sign up! Call outs \u00b6 Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug. Call outs with code \u00b6 Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Formatting \u00b6 In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through Tables \u00b6 OS or Application Username Password Windows VM Administrator foo Linux VM root bar Emojis \u00b6 Yes, these work. Images \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien right align image \u00b6 Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"mkdocs examples"},{"location":"resources/MKDOCS/#mkdocs-examples","text":"This page includes a few neat tricks that you can do with mkdocs . For a complete list of examples visit the mkdocs documentation .","title":"mkdocs examples"},{"location":"resources/MKDOCS/#code","text":"print ( \"hello world!\" )","title":"Code"},{"location":"resources/MKDOCS/#code-with-line-numbers","text":"1 2 3 4 5 def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with line numbers"},{"location":"resources/MKDOCS/#code-with-highlights","text":"def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ]","title":"Code with highlights"},{"location":"resources/MKDOCS/#code-with-tabs","text":"Tab Header #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } Another Tab Header #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Code with tabs"},{"location":"resources/MKDOCS/#more-tabs","text":"Windows If on windows download the Win32.zip file and install it. MacOS Run brew install foo . Linux Run apt-get install foo .","title":"More tabs"},{"location":"resources/MKDOCS/#checklists","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst","title":"Checklists"},{"location":"resources/MKDOCS/#add-a-button","text":"Launch the lab Visit IBM Developer Sign up!","title":"Add a button"},{"location":"resources/MKDOCS/#call-outs","text":"Tip You can use note , abstract , info , tip , success , question warning , failure , danger , bug , quote or example . Note A note. Abstract An abstract. Info Some info. Success A success. Question A question. Warning A warning. Danger A danger. Example A example. Bug A bug.","title":"Call outs"},{"location":"resources/MKDOCS/#call-outs-with-code","text":"Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.","title":"Call outs with code"},{"location":"resources/MKDOCS/#formatting","text":"In addition to the usual italics , and bold there is now support for: highlighted underlined strike-through","title":"Formatting"},{"location":"resources/MKDOCS/#tables","text":"OS or Application Username Password Windows VM Administrator foo Linux VM root bar","title":"Tables"},{"location":"resources/MKDOCS/#emojis","text":"Yes, these work.","title":"Emojis"},{"location":"resources/MKDOCS/#images","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"Images"},{"location":"resources/MKDOCS/#right-align-image","text":"Nunc eu odio eleifend, blandit leo a, volutpat sapien","title":"right align image"},{"location":"resources/RESOURCES/","text":"Additional resources \u00b6 IBM Demos \u00b6 Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage Redbooks \u00b6 IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices Videos \u00b6 Video: Postal codes and part numbers (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Additional resources"},{"location":"resources/RESOURCES/#additional-resources","text":"","title":"Additional resources"},{"location":"resources/RESOURCES/#ibm-demos","text":"Collection: InfoSphere Information Server Tutorial: Transforming your data with IBM DataStage","title":"IBM Demos"},{"location":"resources/RESOURCES/#redbooks","text":"IBM InfoSphere DataStage Data Flow and Job Design InfoSphere DataStage Parallel Framework Standard Practices","title":"Redbooks"},{"location":"resources/RESOURCES/#videos","text":"Video: Postal codes and part numbers (DataStage) Video: Find relationships between sales, employees, and customers (Information Analyzer) Video: Clean and analyze data (Governance Catalog)","title":"Videos"}]}